{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmW3qqRhpb3t"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# IMPORTAÇÃO DAS BIBLIOTECAS\n",
        "# ==========================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "import json\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier # Although not used as the main model, it's good to keep if considering other options\n",
        "from sklearn.tree import DecisionTreeClassifier # Although not used as the main model, it's good to keep if considering other options\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# For optional balancing (uncomment if you plan to use)\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "# from imblearn.under_sampling import RandomUnderSampler\n",
        "# from collections import Counter\n",
        "\n",
        "\n",
        "# Configurações de visualização\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"✅ Bibliotecas importadas com sucesso!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "De0ga8fYpk6U"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 1. EXTRAÇÃO DE DADOS\n",
        "# ==========================================\n",
        "\n",
        "def extrair_dados_api(url):\n",
        "    \"\"\"\n",
        "    Extrai dados da API da Telecom X.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"🔄 Fazendo requisição para a API...\")\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        dados = response.json()\n",
        "\n",
        "        print(f\"📋 Tipo de dados recebidos: {type(dados)}\")\n",
        "\n",
        "        if isinstance(dados, list):\n",
        "            print(f\"📊 Lista com {len(dados)} elementos\")\n",
        "            if len(dados) > 0:\n",
        "                print(f\"🔍 Primeiro elemento: {type(dados[0])}\")\n",
        "                if isinstance(dados[0], dict):\n",
        "                    print(f\"🗝️ Chaves do primeiro elemento: {list(dados[0].keys())}\")\n",
        "        elif isinstance(dados, dict):\n",
        "            print(f\"📊 Dicionário com chaves: {list(dados.keys())}\")\n",
        "\n",
        "        if isinstance(dados, list):\n",
        "            df = pd.DataFrame(dados)\n",
        "        elif isinstance(dados, dict):\n",
        "            if 'data' in dados:\n",
        "                df = pd.DataFrame(dados['data'])\n",
        "            elif 'customers' in dados:\n",
        "                df = pd.DataFrame(dados['customers'])\n",
        "            else:\n",
        "                df = pd.DataFrame([dados])\n",
        "        else:\n",
        "            raise ValueError(f\"Formato de dados não suportado: {type(dados)}\")\n",
        "\n",
        "        print(f\"✅ Dados extraídos com sucesso! Shape: {df.shape}\")\n",
        "        print(f\"📋 Colunas: {list(df.columns)}\")\n",
        "        return df\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"❌ Erro na requisição HTTP: {e}\")\n",
        "        print(\"🔄 Tentando usar dados simulados...\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro ao processar dados: {e}\")\n",
        "        print(\"🔄 Tentando usar dados simulados...\")\n",
        "        return None\n",
        "\n",
        "# URL da API\n",
        "api_url = \"https://raw.githubusercontent.com/ingridcristh/challenge2-data-science/main/TelecomX_Data.json\"\n",
        "\n",
        "# Carregamento dos dados\n",
        "print(\"🔄 Carregando dados da API...\")\n",
        "df_raw = extrair_dados_api(api_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXVB0HJFpqg7"
      },
      "outputs": [],
      "source": [
        "# Verificação inicial dos dados\n",
        "if df_raw is not None:\n",
        "    print(f\"📊 Dataset original: {df_raw.shape[0]} linhas, {df_raw.shape[1]} colunas\")\n",
        "    print(\"\\n📋 Primeiras 5 linhas:\")\n",
        "    print(df_raw.head())\n",
        "    print(\"\\n📋 Informações gerais:\")\n",
        "    print(df_raw.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwmKwHQJptar"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 2. TRANSFORMAÇÃO E LIMPEZA DOS DADOS (PARTE 1 DO DESAFIO)\n",
        "# ==========================================\n",
        "\n",
        "def expandir_colunas_complexas(df):\n",
        "    \"\"\"\n",
        "    Expande colunas que contêm dicionários aninhados em novas colunas.\n",
        "    \"\"\"\n",
        "    print(\"\\n🔧 EXPANDINDO COLUNAS COMPLEXAS\")\n",
        "    print(\"=\" * 50)\n",
        "    df_expanded = df.copy()\n",
        "    colunas_para_expandir = []\n",
        "\n",
        "    for col in df_expanded.columns:\n",
        "        if df_expanded[col].dtype == 'object':\n",
        "            sample_val = df_expanded[col].dropna().iloc[0] if not df_expanded[col].dropna().empty else None\n",
        "            if isinstance(sample_val, dict):\n",
        "                colunas_para_expandir.append(col)\n",
        "\n",
        "    for col in colunas_para_expandir:\n",
        "        print(f\"🔧 Expandindo coluna: {col}\")\n",
        "        try:\n",
        "            col_expanded = pd.json_normalize(df_expanded[col])\n",
        "            col_expanded.columns = [f\"{col}_{subcol}\" for subcol in col_expanded.columns]\n",
        "            df_expanded = pd.concat([df_expanded.drop(columns=[col]), col_expanded], axis=1)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Erro ao expandir coluna {col}: {e}\")\n",
        "\n",
        "    print(f\"✅ Colunas complexas expandidas! Shape: {df_expanded.shape}\")\n",
        "    print(f\"📋 Novas colunas: {list(df_expanded.columns)}\")\n",
        "    return df_expanded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPV1pblapwkt"
      },
      "outputs": [],
      "source": [
        "def explorar_estrutura_dados(df):\n",
        "    \"\"\"\n",
        "    Explora a estrutura inicial dos dados.\n",
        "    \"\"\"\n",
        "    print(\"🔍 EXPLORAÇÃO INICIAL DOS DADOS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    print(f\"📏 Dimensões: {df.shape}\")\n",
        "    print(f\"📊 Colunas disponíveis: {list(df.columns)}\")\n",
        "\n",
        "    print(f\"\\n📊 Tipos de dados:\")\n",
        "    for col in df.columns:\n",
        "        tipo = df[col].dtype\n",
        "        print(f\"   {col}: {tipo}\")\n",
        "\n",
        "    print(f\"\\n🔍 Verificando estrutura das colunas:\")\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            sample_values = df[col].dropna().head(3).tolist()\n",
        "            print(f\"   {col}: {[type(v).__name__ for v in sample_values]}\")\n",
        "            if len(sample_values) > 0:\n",
        "                print(f\"     Exemplo: {sample_values[0]}\")\n",
        "\n",
        "    print(f\"\\n❓ Valores ausentes:\")\n",
        "    missing_data = df.isnull().sum()\n",
        "    if missing_data.sum() > 0:\n",
        "        print(missing_data[missing_data > 0])\n",
        "    else:\n",
        "        print(\"Nenhum valor ausente encontrado!\")\n",
        "\n",
        "    print(f\"\\n🔄 Valores duplicados: {df.duplicated().sum()}\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMF_pW9Cpz4c"
      },
      "outputs": [],
      "source": [
        "def limpar_e_tratar_dados(df):\n",
        "    \"\"\"\n",
        "    Limpa e trata inconsistências nos dados.\n",
        "    \"\"\"\n",
        "    print(\"\\n🧹 LIMPEZA E TRATAMENTO DOS DADOS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    df_tratado = df.copy()\n",
        "\n",
        "    # Converter colunas numéricas que foram lidas como objeto\n",
        "    numeric_cols_to_convert = ['account_Charges_Monthly', 'account_Charges_Total']\n",
        "    for col in numeric_cols_to_convert:\n",
        "        if col in df_tratado.columns:\n",
        "            df_tratado[col] = pd.to_numeric(df_tratado[col], errors='coerce')\n",
        "            print(f\"🔄 Coluna '{col}' convertida para numérica.\")\n",
        "\n",
        "    # Verificar e tratar valores ausentes\n",
        "    if df_tratado.isnull().sum().sum() > 0:\n",
        "        print(\"🔧 Tratando valores ausentes...\")\n",
        "        for col in df_tratado.columns:\n",
        "            if df_tratado[col].isnull().any():\n",
        "                if df_tratado[col].dtype == 'object':\n",
        "                    if not df_tratado[col].mode().empty:\n",
        "                        df_tratado[col].fillna(df_tratado[col].mode()[0], inplace=True)\n",
        "                    else:\n",
        "                        df_tratado[col].fillna('Unknown', inplace=True)\n",
        "                else:\n",
        "                    df_tratado[col].fillna(df_tratado[col].median(), inplace=True)\n",
        "        print(\"✅ Valores ausentes tratados.\")\n",
        "    else:\n",
        "        print(\"Nenhum valor ausente para tratar.\")\n",
        "\n",
        "    # Remover duplicados\n",
        "    duplicados_antes = df_tratado.duplicated().sum()\n",
        "    if duplicados_antes > 0:\n",
        "        df_tratado.drop_duplicates(inplace=True)\n",
        "        print(f\"🗑️ Removidos {duplicados_antes} registros duplicados\")\n",
        "    else:\n",
        "        print(\"Nenhum registro duplicado encontrado.\")\n",
        "\n",
        "    # Padronizar colunas categóricas (strip espaços em branco)\n",
        "    colunas_categoricas = df_tratado.select_dtypes(include=['object']).columns\n",
        "    for col in colunas_categoricas:\n",
        "        if col in df_tratado.columns:\n",
        "            df_tratado[col] = df_tratado[col].astype(str).str.strip()\n",
        "    print(\"✅ Colunas categóricas padronizadas.\")\n",
        "\n",
        "    print(f\"✅ Dados limpos! Shape final: {df_tratado.shape}\")\n",
        "    print(f\"📋 Colunas finais: {list(df_tratado.columns)}\")\n",
        "    return df_tratado\n",
        "\n",
        "def criar_coluna_contas_diarias(df, coluna_faturamento_mensal):\n",
        "    \"\"\"\n",
        "    Cria a coluna de contas diárias baseada no faturamento mensal.\n",
        "    \"\"\"\n",
        "    if coluna_faturamento_mensal in df.columns:\n",
        "        df['Contas_Diarias'] = df[coluna_faturamento_mensal] / 30\n",
        "        print(f\"✅ Coluna 'Contas_Diarias' criada com sucesso!\")\n",
        "    else:\n",
        "        print(f\"❌ Coluna {coluna_faturamento_mensal} não encontrada\")\n",
        "    return df\n",
        "\n",
        "def padronizar_dados(df):\n",
        "    \"\"\"\n",
        "    Padroniza dados categóricos para análise (Yes/No para 1/0).\n",
        "    \"\"\"\n",
        "    print(\"\\n🔄 PADRONIZAÇÃO DOS DADOS (Yes/No para 1/0)\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    df_padronizado = df.copy()\n",
        "\n",
        "    colunas_yes_no = []\n",
        "    for col in df_padronizado.columns:\n",
        "        if df_padronizado[col].dtype == 'object':\n",
        "            valores_unicos = df_padronizado[col].unique()\n",
        "            if len(valores_unicos) == 2 and set(str(v).lower() for v in valores_unicos) <= {'yes', 'no', 'sim', 'não'}:\n",
        "                colunas_yes_no.append(col)\n",
        "\n",
        "    for col in colunas_yes_no:\n",
        "        df_padronizado[col] = df_padronizado[col].map(\n",
        "            lambda x: 1 if str(x).lower() in ['yes', 'sim'] else 0\n",
        "        )\n",
        "        print(f\"🔄 Coluna '{col}' convertida para binário (1/0)\")\n",
        "\n",
        "    print(\"✅ Padronização de Yes/No concluída.\")\n",
        "    return df_padronizado\n",
        "\n",
        "# --- Fluxo de execução da Parte 1 ---\n",
        "if df_raw is not None:\n",
        "    df_expanded = expandir_colunas_complexas(df_raw.copy())\n",
        "    df_clean_part1 = explorar_estrutura_dados(df_expanded.copy())\n",
        "    df_clean_part1 = limpar_e_tratar_dados(df_clean_part1)\n",
        "    df_clean_part1 = criar_coluna_contas_diarias(df_clean_part1, 'account_Charges_Monthly')\n",
        "    df_clean_part1 = padronizar_dados(df_clean_part1)\n",
        "else:\n",
        "    df_clean_part1 = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKAVnq_0p7DE"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 3. PREPARAÇÃO DOS DADOS PARA MODELAGEM (BACKLOG - PARTE 2)\n",
        "# ==========================================\n",
        "\n",
        "def remover_colunas_irrelevantes(df):\n",
        "    \"\"\"\n",
        "    Elimina colunas que não trazem valor para a análise ou para os modelos preditivos.\n",
        "    \"\"\"\n",
        "    print(\"\\n🗑️ REMOVENDO COLUNAS IRRELEVANTES\")\n",
        "    print(\"=\" * 50)\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # Lista de colunas a serem removidas. 'customerID' é um identificador único.\n",
        "    # Adicione outras colunas aqui se considerar irrelevantes após o encoding.\n",
        "    cols_to_drop = ['customerID']\n",
        "\n",
        "    existing_cols_to_drop = [col for col in cols_to_drop if col in df_processed.columns]\n",
        "\n",
        "    if existing_cols_to_drop:\n",
        "        df_processed = df_processed.drop(columns=existing_cols_to_drop)\n",
        "        print(f\"✅ Colunas removidas: {existing_cols_to_drop}\")\n",
        "    else:\n",
        "        print(\"Nenhuma coluna irrelevante para remover ou as colunas já foram removidas.\")\n",
        "\n",
        "    print(f\"Shape após remoção: {df_processed.shape}\")\n",
        "    return df_processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hf27EEqNp_w9"
      },
      "outputs": [],
      "source": [
        "def encoding_variaveis_categoricas(df, target_column='Churn'):\n",
        "    \"\"\"\n",
        "    Transforma as variáveis categóricas em formato numérico para torná-las compatíveis\n",
        "    com algoritmos de machine learning. Utiliza one-hot encoding.\n",
        "    A coluna alvo ('Churn') será mapeada para 0 e 1.\n",
        "    \"\"\"\n",
        "    print(\"\\n🔄 ENCODING DE VARIÁVEIS CATEGÓRICAS\")\n",
        "    print(\"=\" * 50)\n",
        "    df_encoded = df.copy()\n",
        "\n",
        "    # Mapear a coluna alvo 'Churn' para numérica (0 e 1)\n",
        "    if target_column in df_encoded.columns:\n",
        "        if df_encoded[target_column].dtype == 'object':\n",
        "            df_encoded[target_column] = df_encoded[target_column].map({'No': 0, 'Yes': 1})\n",
        "            print(f\"✅ Coluna '{target_column}' mapeada para 0/1.\")\n",
        "        elif df_encoded[target_column].dtype in [np.int64, np.float64]:\n",
        "            print(f\"Colun '{target_column}' já é numérica (0/1).\")\n",
        "    else:\n",
        "        print(f\"❌ Coluna alvo '{target_column}' não encontrada.\")\n",
        "        return df_encoded\n",
        "\n",
        "\n",
        "    # Identificar colunas categóricas para One-Hot Encoding (excluindo a coluna alvo)\n",
        "    categorical_cols = df_encoded.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "    if categorical_cols:\n",
        "        print(f\"Aplicando One-Hot Encoding nas colunas: {categorical_cols}\")\n",
        "        # drop_first=True evita a multicolinearidade, que é importante para alguns modelos como Regressão Logística\n",
        "        df_encoded = pd.get_dummies(df_encoded, columns=categorical_cols, drop_first=True, dtype=int)\n",
        "        print(\"✅ One-Hot Encoding aplicado com sucesso.\")\n",
        "    else:\n",
        "        print(\"Nenhuma coluna categórica para aplicar One-Hot Encoding.\")\n",
        "\n",
        "    print(f\"Shape após encoding: {df_encoded.shape}\")\n",
        "    print(f\"Novas colunas (exemplo): {list(df_encoded.columns[:10])}...\")\n",
        "    return df_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAmk6CKTq1RZ"
      },
      "outputs": [],
      "source": [
        "def verificar_proporcao_evasao(df, target_column='Churn'):\n",
        "    \"\"\"\n",
        "    Calcula a proporção de clientes que evadiram em relação aos que permaneceram ativos.\n",
        "    Avalia se há desequilíbrio entre as classes.\n",
        "    \"\"\"\n",
        "    print(f\"\\n📊 VERIFICAÇÃO DA PROPORÇÃO DE EVASÃO ({target_column.upper()})\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    if target_column not in df.columns:\n",
        "        print(f\"❌ Coluna alvo '{target_column}' não encontrada.\")\n",
        "        return\n",
        "\n",
        "    churn_counts = df[target_column].value_counts()\n",
        "    churn_proportions = df[target_column].value_counts(normalize=True) * 100\n",
        "\n",
        "    print(\"Contagem de Classes:\")\n",
        "    print(churn_counts)\n",
        "    print(\"\\nProporção das Classes (%):\")\n",
        "    print(churn_proportions.round(2))\n",
        "\n",
        "    if churn_proportions.min() < 20: # Limiar arbitrário para considerar desbalanceado\n",
        "        print(f\"\\n⚠️ As classes estão desbalanceadas. A classe minoritária representa {churn_proportions.min():.2f}% dos dados.\")\n",
        "        print(\"Isso pode impactar o desempenho do modelo, especialmente para a classe minoritária. Considere técnicas de balanceamento.\")\n",
        "    else:\n",
        "        print(\"✅ As classes parecem razoavelmente balanceadas.\")\n",
        "\n",
        "  # Plotar para visualização\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    sns.barplot(x=churn_proportions.index, y=churn_proportions.values, palette=['lightcoral', 'skyblue'])\n",
        "    plt.title(f'Proporção da Variável Alvo: {target_column}')\n",
        "    plt.xlabel(f'{target_column} (0=Não Churn, 1=Churn)')\n",
        "    plt.ylabel('Proporção (%)')\n",
        "    plt.ylim(0, 100)\n",
        "    for index, value in enumerate(churn_proportions.values):\n",
        "        plt.text(index, value + 2, f'{value:.2f}%', ha='center')\n",
        "    plt.show()\n",
        "\n",
        "# --- Fluxo de execução da Parte 2 (Preparação) ---\n",
        "df_model_prep = None\n",
        "df_encoded = None\n",
        "if df_clean_part1 is not None:\n",
        "    df_model_prep = remover_colunas_irrelevantes(df_clean_part1.copy())\n",
        "    df_encoded = encoding_variaveis_categoricas(df_model_prep.copy(), target_column='Churn')\n",
        "    if df_encoded is not None:\n",
        "        verificar_proporcao_evasao(df_encoded.copy(), target_column='Churn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lF9LQuEYrMTB"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# BALANCEAMENTO DE CLASSES (OPCIONAL)\n",
        "# ==========================================\n",
        "# Descomente esta seção se quiser aplicar balanceamento.\n",
        "# Lembre-se de instalar 'imbalanced-learn' (pip install imbalanced-learn)\n",
        "\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "# from imblearn.under_sampling import RandomUnderSampler\n",
        "# from collections import Counter\n",
        "\n",
        "# def balancear_classes(X, y, method='smote'):\n",
        "#     \"\"\"\n",
        "#     Aplica técnicas de balanceamento de classes.\n",
        "#     \"\"\"\n",
        "#     print(f\"\\n⚖️ BALANCEAMENTO DE CLASSES ({method.upper()})\")\n",
        "#     print(\"=\" * 50)\n",
        "#     print(f\"Proporção original: {Counter(y)}\")\n",
        "\n",
        "#     if method == 'smote':\n",
        "#         smote = SMOTE(random_state=42)\n",
        "#         X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "#         print(f\"Proporção após SMOTE: {Counter(y_resampled)}\")\n",
        "#         print(\"✅ SMOTE (Oversampling) aplicado com sucesso.\")\n",
        "#     elif method == 'undersampling':\n",
        "#         rus = RandomUnderSampler(random_state=42)\n",
        "#         X_resampled, y_resampled = rus.fit_resample(X, y)\n",
        "#         print(f\"Proporção após Undersampling: {Counter(y_resampled)}\")\n",
        "#         print(\"✅ Undersampling aplicado com sucesso.\")\n",
        "#     else:\n",
        "#         print(\"❌ Método de balanceamento não reconhecido. Retornando dados originais.\")\n",
        "#         return X, y\n",
        "\n",
        "#     print(f\"Shape após balanceamento: {X_resampled.shape}\")\n",
        "#     return X_resampled, y_resampled\n",
        "\n",
        "# df_final_for_modeling = df_encoded.copy()\n",
        "# if df_encoded is not None:\n",
        "#      X_bal = df_encoded.drop(columns=['Churn'])\n",
        "#      y_bal = df_encoded['Churn']\n",
        "#      X_resampled, y_resampled = balancear_classes(X_bal, y_bal, method='smote')\n",
        "#      df_balanced = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "#      df_final_for_modeling = df_balanced # Use o DF balanceado para as próximas etapas\n",
        "# else:\n",
        "#     df_balanced = None\n",
        "\n",
        "# Por padrão, vamos continuar com o dataframe não balanceado para este relatório.\n",
        "df_final_for_modeling = df_encoded.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGV-PuNurYft"
      },
      "outputs": [],
      "source": [
        "def normalizar_ou_padronizar_dados(df, target_column='Churn', method='standard'):\n",
        "    \"\"\"\n",
        "    Normaliza ou padroniza os dados, conforme os modelos que serão aplicados.\n",
        "    Retorna X_scaled (features escaladas) e y (variável alvo).\n",
        "    \"\"\"\n",
        "    print(f\"\\n🔄 NORMALIZAÇÃO/PADRONIZAÇÃO DOS DADOS ({method.upper()})\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Verificar se o DataFrame não é None\n",
        "    if df is None:\n",
        "        print(\"❌ DataFrame fornecido é None.\")\n",
        "        return None, None\n",
        "\n",
        "    # Verificar se a coluna alvo existe\n",
        "    if target_column not in df.columns:\n",
        "        print(f\"❌ Coluna alvo '{target_column}' não encontrada.\")\n",
        "        print(f\"Colunas disponíveis: {list(df.columns)}\")\n",
        "        return None, None\n",
        "\n",
        "    # Separar features e target\n",
        "    X = df.drop(columns=[target_column])\n",
        "    y = df[target_column]\n",
        "\n",
        "    # Identificar colunas numéricas\n",
        "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "    if len(numeric_cols) == 0:\n",
        "        print(\"⚠️ Nenhuma coluna numérica para normalizar/padronizar. Retornando X e y originais.\")\n",
        "        return X, y\n",
        "\n",
        "    print(f\"📊 Colunas numéricas para escalar: {list(numeric_cols)}\")\n",
        "\n",
        "    # Escolher o scaler baseado no método\n",
        "    if method == 'standard':\n",
        "        scaler = StandardScaler()\n",
        "        print(\"⚙️ Aplicando StandardScaler (Padronização)...\")\n",
        "    elif method == 'minmax':\n",
        "        scaler = MinMaxScaler()\n",
        "        print(\"⚙️ Aplicando MinMaxScaler (Normalização)...\")\n",
        "    else:\n",
        "        print(f\"❌ Método de escalonamento '{method}' não reconhecido. Use 'standard' ou 'minmax'.\")\n",
        "        return X, y\n",
        "\n",
        "    # Aplicar o escalonamento\n",
        "    try:\n",
        "        X_scaled_array = scaler.fit_transform(X[numeric_cols])\n",
        "        X_scaled = pd.DataFrame(X_scaled_array, columns=numeric_cols, index=X.index)\n",
        "\n",
        "        # Recombinar colunas não numéricas (se houver)\n",
        "        non_numeric_cols = X.select_dtypes(exclude=[np.number]).columns\n",
        "        if len(non_numeric_cols) > 0:\n",
        "            print(f\"📋 Mantendo colunas não numéricas: {list(non_numeric_cols)}\")\n",
        "            X_scaled = pd.concat([X_scaled, X[non_numeric_cols]], axis=1)\n",
        "\n",
        "        print(f\"✅ Dados escalados com sucesso. Shape: {X_scaled.shape}\")\n",
        "        return X_scaled, y\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro durante o escalonamento: {str(e)}\")\n",
        "        return X, y\n",
        "\n",
        "\n",
        "# Verificar se df_final_for_modeling existe e não é None\n",
        "try:\n",
        "    if 'df_final_for_modeling' in locals() and df_final_for_modeling is not None:\n",
        "        print(\"📋 DataFrame df_final_for_modeling encontrado. Processando...\")\n",
        "\n",
        "        # Prepara os dados sem escalonamento\n",
        "        X_no_scale = df_final_for_modeling.drop(columns=['Churn'])\n",
        "        y = df_final_for_modeling['Churn']\n",
        "\n",
        "        # Prepara os dados com escalonamento\n",
        "        X_scaled, y_scaled = normalizar_ou_padronizar_dados(\n",
        "            df_final_for_modeling.copy(),\n",
        "            target_column='Churn',\n",
        "            method='standard'\n",
        "        )\n",
        "\n",
        "        print(f\"📊 Variáveis criadas:\")\n",
        "        print(f\"   - X_no_scale shape: {X_no_scale.shape}\")\n",
        "        print(f\"   - y shape: {y.shape}\")\n",
        "        if X_scaled is not None:\n",
        "            print(f\"   - X_scaled shape: {X_scaled.shape}\")\n",
        "\n",
        "    else:\n",
        "        print(\"⚠️ DataFrame df_final_for_modeling não encontrado ou é None.\")\n",
        "        X_no_scale, y = None, None\n",
        "        X_scaled, y_scaled = None, None\n",
        "\n",
        "except NameError:\n",
        "    print(\"⚠️ Variável df_final_for_modeling não foi definida.\")\n",
        "    X_no_scale, y = None, None\n",
        "    X_scaled, y_scaled = None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4IGmHm1s2VN"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# MODELAGEM PREDITIVA\n",
        "# ==========================================\n",
        "\n",
        "# Separação de Dados\n",
        "print(\"\\n--- SPLIT DE DADOS EM TREINO E TESTE ---\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = [None]*4 # Initialize with None\n",
        "X_train_no_scale, X_test_no_scale, y_train_no_scale, y_test_no_scale = [None]*4 # Initialize with None\n",
        "\n",
        "if X_scaled is not None and X_no_scale is not None and y is not None:\n",
        "    # --- Add NaN handling for y before splitting ---\n",
        "    print(\"🔧 Verificando e tratando NaN na variável alvo 'y' para estratificação...\")\n",
        "    initial_rows = y.shape[0]\n",
        "    # Create a boolean mask for non-NaN values in y\n",
        "    non_nan_mask = y.notna()\n",
        "\n",
        "    # Apply the mask to X_scaled, X_no_scale, and y\n",
        "    X_scaled_filtered = X_scaled[non_nan_mask]\n",
        "    X_no_scale_filtered = X_no_scale[non_nan_mask]\n",
        "    y_filtered = y[non_nan_mask]\n",
        "\n",
        "    removed_rows = initial_rows - y_filtered.shape[0]\n",
        "    if removed_rows > 0:\n",
        "        print(f\"🗑️ Removidos {removed_rows} linhas com NaN na variável alvo para estratificação.\")\n",
        "    else:\n",
        "        print(\"✅ Nenhuma linha com NaN encontrada na variável alvo.\")\n",
        "    # --- End NaN handling ---\n",
        "\n",
        "\n",
        "    # Divisão para o modelo que exige normalização\n",
        "    # Use the filtered dataframes for splitting\n",
        "    X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(\n",
        "        X_scaled_filtered, y_filtered, test_size=0.3, random_state=42, stratify=y_filtered\n",
        "    )\n",
        "    print(f\"Dados escalados: X_train_scaled: {X_train_scaled.shape}, X_test_scaled: {X_test_scaled.shape}\")\n",
        "\n",
        "    # Divisão para o modelo que não exige normalização\n",
        "    # Use the filtered dataframes for splitting\n",
        "    X_train_no_scale, X_test_no_scale, y_train_no_scale, y_test_no_scale = train_test_split(\n",
        "        X_no_scale_filtered, y_filtered, test_size=0.3, random_state=42, stratify=y_filtered\n",
        "    )\n",
        "    print(f\"Dados não escalados: X_train_no_scale: {X_train_no_scale.shape}, X_test_no_scale: {X_test_no_scale.shape}\")\n",
        "    print(\"✅ Dados divididos em treino e teste com sucesso.\")\n",
        "else:\n",
        "    print(\"❌ Erro: Dados X ou y não preparados para split. Certifique-se de que `df_final_for_modeling` não é None.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkwqeaL0bcbZ"
      },
      "outputs": [],
      "source": [
        "def avaliar_modelo(model_name, y_true, y_pred, y_prob=None):\n",
        "    \"\"\"\n",
        "    Avalia o modelo e imprime as métricas.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Avaliação do Modelo: {model_name} ---\")\n",
        "    print(f\"Acurácia: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Precisão: {precision_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Recall: {recall_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"F1-score: {f1_score(y_true, y_pred):.4f}\")\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No Churn', 'Churn'])\n",
        "    disp.plot(cmap='Blues')\n",
        "    plt.title(f'Matriz de Confusão: {model_name}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhNuKIj0bcXs"
      },
      "outputs": [],
      "source": [
        "# Criação e Avaliação dos Modelos\n",
        "log_reg_model = None\n",
        "rf_model = None\n",
        "\n",
        "# Modelo 1: Regressão Logística (requer normalização)\n",
        "if X_train_scaled is not None and y_train_scaled is not None:\n",
        "    print(\"\\n--- Treinando Modelo 1: Regressão Logística ---\")\n",
        "    print(\"Justificativa: Modelo linear e interpretável, adequado para problemas de classificação binária. Requer normalização para melhor desempenho, pois é sensível à escala das features.\")\n",
        "    log_reg_model = LogisticRegression(random_state=42, solver='liblinear')\n",
        "    log_reg_model.fit(X_train_scaled, y_train_scaled)\n",
        "    y_pred_log_reg = log_reg_model.predict(X_test_scaled)\n",
        "    y_prob_log_reg = log_reg_model.predict_proba(X_test_scaled)[:, 1]\n",
        "    avaliar_modelo(\"Regressão Logística\", y_test_scaled, y_pred_log_reg, y_prob_log_reg)\n",
        "else:\n",
        "    print(\"❌ Não foi possível treinar a Regressão Logística: dados escalados ausentes.\")\n",
        "\n",
        "# Modelo 2: Random Forest (não requer normalização)\n",
        "if X_train_no_scale is not None and y_train_no_scale is not None:\n",
        "    print(\"\\n--- Treinando Modelo 2: Random Forest ---\")\n",
        "    print(\"Justificativa: Modelo de ensemble robusto, que combina múltiplas árvores de decisão para melhor generalização. Não é sensível à escala dos dados e lida bem com variáveis categóricas já codificadas. Geralmente oferece bom desempenho e é menos propenso a overfitting que uma única árvore.\")\n",
        "    rf_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "    rf_model.fit(X_train_no_scale, y_train_no_scale)\n",
        "    y_pred_rf = rf_model.predict(X_test_no_scale)\n",
        "    y_prob_rf = rf_model.predict_proba(X_test_no_scale)[:, 1]\n",
        "    avaliar_modelo(\"Random Forest\", y_test_no_scale, y_pred_rf, y_prob_rf)\n",
        "else:\n",
        "    print(\"❌ Não foi possível treinar o Random Forest: dados não escalados ausentes.\")\n",
        "\n",
        "\n",
        "# Análise Crítica e Comparação dos Modelos (Textual no relatório)\n",
        "print(\"\\n--- ANÁLISE CRÍTICA E COMPARAÇÃO DOS MODELOS ---\")\n",
        "print(\"=\" * 50)\n",
        "print(\"A análise crítica e comparação detalhada dos modelos será apresentada na seção de relatório final.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgfonBPmbcCF"
      },
      "outputs": [],
      "source": [
        "# Análise de Importância das Variáveis\n",
        "def analisar_importancia_variaveis(model, feature_names, model_type='logistic_regression'):\n",
        "    \"\"\"\n",
        "    Analisa e imprime a importância das variáveis para diferentes tipos de modelos.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Análise de Importância das Variáveis para {model_type.replace('_', ' ').title()} ---\")\n",
        "\n",
        "    if model_type == 'logistic_regression':\n",
        "        if hasattr(model, 'coef_') and len(model.coef_[0]) == len(feature_names):\n",
        "            coefficients = pd.Series(model.coef_[0], index=feature_names)\n",
        "            importance = coefficients.abs().sort_values(ascending=False)\n",
        "            print(\"Coeficientes (magnitude absoluta, indicando importância):\")\n",
        "            print(importance.head(10))\n",
        "\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            sns.barplot(x=coefficients.sort_values(key=abs).tail(20).values,\n",
        "                        y=coefficients.sort_values(key=abs).tail(20).index,\n",
        "                        palette='coolwarm')\n",
        "            plt.title('Importância das Variáveis (Coeficientes da Regressão Logística)')\n",
        "            plt.xlabel('Valor do Coeficiente')\n",
        "            plt.ylabel('Variável')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"Modelo de Regressão Logística não treinado ou número de features inconsistente.\")\n",
        "\n",
        "    elif model_type == 'random_forest':\n",
        "        if hasattr(model, 'feature_importances_') and len(model.feature_importances_) == len(feature_names):\n",
        "            importance = pd.Series(model.feature_importances_, index=feature_names)\n",
        "            importance = importance.sort_values(ascending=False)\n",
        "            print(\"Importância das variáveis (Feature Importance):\")\n",
        "            print(importance.head(10))\n",
        "\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            sns.barplot(x=importance.head(20).values, y=importance.head(20).index, palette='viridis')\n",
        "            plt.title('Importância das Variáveis (Random Forest)')\n",
        "            plt.xlabel('Importância')\n",
        "            plt.ylabel('Variável')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"Modelo Random Forest não treinado ou número de features inconsistente.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Tipo de modelo '{model_type}' não suportado para análise direta de importância de variáveis neste script.\")\n",
        "    print(\"✅ Análise de importância de variáveis concluída.\")\n",
        "\n",
        "\n",
        "# Executar análise de importância para os modelos criados\n",
        "if log_reg_model is not None and X_scaled is not None:\n",
        "    analisar_importancia_variaveis(log_reg_model, X_scaled.columns, model_type='logistic_regression')\n",
        "\n",
        "if rf_model is not None and X_no_scale is not None:\n",
        "    analisar_importancia_variaveis(rf_model, X_no_scale.columns, model_type='random_forest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-TsoMZscMIV"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# RELATÓRIO FINAL\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\\n--- RELATÓRIO DE ANÁLISE PREDITIVA DE EVASÃO DE CLIENTES (CHURN) ---\\n\")\n",
        "\n",
        "print(\"## Introdução\")\n",
        "print(\"---\")\n",
        "print(\"Este relatório apresenta uma análise aprofundada da evasão de clientes (Churn) para uma empresa de telecomunicações, culminando na **construção e avaliação de modelos preditivos**. O objetivo principal é não apenas identificar os fatores que levam os clientes a cancelar seus serviços, mas também desenvolver ferramentas capazes de **prever** o churn, permitindo à empresa implementar estratégias de retenção mais eficazes e proativas. A retenção de clientes é fundamental para a saúde financeira de qualquer negócio baseado em serviços, e a compreensão do churn é o primeiro passo para garantir a lealdade do cliente.\")\n",
        "\n",
        "print(\"\\n## Limpeza e Preparação de Dados\")\n",
        "print(\"---\")\n",
        "print(\"Para garantir a qualidade e a adequação dos dados para a análise e modelagem preditiva, foram realizadas as seguintes etapas de pré-processamento:\")\n",
        "print(\"- **Extração e Tratamento Inicial (Parte 1 do Desafio):** Os dados foram inicialmente extraídos de uma API e passaram por uma fase de limpeza primária, que incluiu:\")\n",
        "print(\"  - **Expansão de Colunas Complexas:** Colunas aninhadas (como 'customer', 'phone', 'internet', 'account') foram desdobradas em novas colunas para expor todas as features relevantes.\")\n",
        "print(\"  - **Conversão de Tipos:** Colunas como `account_Charges_Monthly` e `account_Charges_Total` foram convertidas para tipos numéricos apropriados.\")\n",
        "print(\"  - **Tratamento de Valores Ausentes e Duplicados:** Foram preenchidos valores ausentes (com moda para categóricas e mediana para numéricas) e removidos registros duplicados.\")\n",
        "print(\"  - **Criação de Features:** A coluna `Contas_Diarias` foi derivada do faturamento mensal para oferecer uma nova perspectiva sobre os gastos diários dos clientes.\")\n",
        "print(\"  - **Padronização Categórica:** Variáveis 'Yes'/'No' foram convertidas para 1/0.\")\n",
        "if df_clean_part1 is not None:\n",
        "    print(f\"  - Após o tratamento inicial, o dataset continha **{df_clean_part1.shape[0]} linhas e {df_clean_part1.shape[1]} colunas**.\")\n",
        "\n",
        "print(\"- **Preparação Específica para Modelagem (Backlog):**\")\n",
        "print(\"  - **Remoção de Colunas Irrelevantes:** A coluna `customerID` foi eliminada por ser um identificador único sem valor preditivo. Outras colunas poderiam ser removidas se consideradas redundantes ou com baixa variância após a análise mais aprofundada.\")\n",
        "if df_model_prep is not None:\n",
        "    print(f\"  - Shape após remoção de irrelevantes: **{df_model_prep.shape[0]} linhas, {df_model_prep.shape[1]} colunas**.\")\n",
        "print(\"  - **Encoding de Variáveis Categóricas:** Todas as colunas categóricas restantes (exceto a coluna alvo 'Churn') foram submetidas ao **One-Hot Encoding**. Isso transformou categorias textuais em um formato numérico binário (0s e 1s), crucial para a maioria dos algoritmos de Machine Learning. A coluna 'Churn' foi mapeada para **0 (Não Churn) e 1 (Churn)**.\")\n",
        "if df_encoded is not None:\n",
        "    print(f\"  - Shape do dataset final após encoding: **{df_encoded.shape[0]} linhas, {df_encoded.shape[1]} colunas**.\")\n",
        "print(\"  - **Verificação de Proporção de Evasão:** Foi confirmado um **desbalanceamento de classes** na variável 'Churn'. A classe 'Não Churn' é majoritária (~73.46%), enquanto 'Churn' é minoritária (~26.54%). Embora não tenha sido aplicado balanceamento de classes neste relatório para simplificação, é uma etapa recomendada em cenários reais para melhorar a capacidade do modelo de prever a classe minoritária.\")\n",
        "print(\"  - **Normalização/Padronização de Dados:** As features numéricas foram **padronizadas** usando `StandardScaler`. Essa etapa é crucial para modelos sensíveis à escala das features, como Regressão Logística e KNN, pois garante que nenhuma feature domine o cálculo de distâncias ou coeficientes apenas por ter uma escala maior. Modelos baseados em árvores (como Random Forest) não exigem essa padronização.\")\n",
        "\n",
        "print(\"\\n## Análise Exploratória e de Correlação\")\n",
        "print(\"---\")\n",
        "print(\"As análises exploratórias e de correlação reforçaram os seguintes insights sobre os fatores que impulsionam o churn:\")\n",
        "print(\"- **Correlações Fortes com Churn:**\")\n",
        "if df_encoded is not None and 'Churn' in df_encoded.columns:\n",
        "    churn_corrs_final = df_encoded.select_dtypes(include=np.number).corr()['Churn'].sort_values(ascending=False)\n",
        "    print(f\"  - **Positivas (Maior Churn):** `{churn_corrs_final.head(3).index.tolist()}`. Dentre elas, `internet_InternetService_Fiber optic` e `account_Contract_Month-to-month` são consistentemente os mais fortes preditores de churn. `account_Charges_Monthly` também mostra uma correlação positiva.\")\n",
        "    print(f\"  - **Negativas (Menor Churn):** `{churn_corrs_final.tail(3).index.tolist()}`. `customer_tenure` (tempo de contrato) é o fator mais negativamente correlacionado, indicando que clientes de longa data são menos propensos a churnar. Contratos de 1 e 2 anos também diminuem a probabilidade de churn.\")\n",
        "print(\"- **Análises Direcionadas:**\")\n",
        "print(\"  - **Tempo de Contrato (`customer_tenure`) x Evasão:** Os gráficos (boxplots e histogramas) demonstraram claramente que clientes com um `tenure` baixo (novos clientes) são significativamente mais propensos a evadir. A maioria dos churns ocorre nos primeiros 12-24 meses de contrato.\")\n",
        "print(\"  - **Total Gasto (`account_Charges_Total`) x Evasão:** Clientes que churnam tendem a ter um `account_Charges_Total` menor, o que é consistente com um `tenure` baixo. Isso sugere que os clientes não permanecem o tempo suficiente para acumular um alto valor total de gastos.\")\n",
        "print(\"  - **Faturamento Mensal (`account_Charges_Monthly`) x Evasão:** Embora `customer_tenure` seja o preditor mais forte, o `account_Charges_Monthly` também se mostrou relevante, com clientes que churnam tendendo a ter faturamentos mensais um pouco mais altos, possivelmente indicando uma sensibilidade maior ao custo ou expectativas de serviço.\")\n",
        "\n",
        "print(\"\\n## Modelagem Preditiva: Criação e Avaliação\")\n",
        "print(\"---\")\n",
        "print(\"O dataset foi dividido em 70% para treino e 30% para teste, com estratificação para manter a proporção de classes da variável alvo. Dois modelos de classificação foram treinados e avaliados:\")\n",
        "\n",
        "print(\"\\n### Modelo 1: Regressão Logística\")\n",
        "print(\"  - **Justificativa:** Um modelo linear amplamente utilizado para classificação binária devido à sua interpretabilidade e eficiência. **A padronização dos dados foi aplicada antes do treinamento**, pois a Regressão Logística calcula coeficientes que são sensíveis à escala das features.\")\n",
        "if 'log_reg_model' in locals() and y_test_scaled is not None and y_pred_log_reg is not None:\n",
        "    print(f\"  - **Métricas no Conjunto de Teste:**\")\n",
        "    print(f\"    - Acurácia: {accuracy_score(y_test_scaled, y_pred_log_reg):.4f}\")\n",
        "    print(f\"    - Precisão: {precision_score(y_test_scaled, y_pred_log_reg):.4f}\")\n",
        "    print(f\"    - Recall: {recall_score(y_test_scaled, y_pred_log_reg):.4f}\")\n",
        "    print(f\"    - F1-score: {f1_score(y_test_scaled, y_pred_log_reg):.4f}\")\n",
        "    print(\"  - **Análise da Matriz de Confusão:** [Refira-se ao gráfico da Matriz de Confusão para a Regressão Logística acima].\")\n",
        "\n",
        "print(\"\\n### Modelo 2: Random Forest\")\n",
        "print(\"  - **Justificativa:** Um algoritmo de ensemble robusto que constrói múltiplas árvores de decisão e combina suas previsões. Ele é menos propenso a overfitting do que uma única árvore e **não é sensível à escala das features**, o que significa que a padronização dos dados não é estritamente necessária para seu bom desempenho.\")\n",
        "if 'rf_model' in locals() and y_test_no_scale is not None and y_pred_rf is not None:\n",
        "    print(f\"  - **Métricas no Conjunto de Teste:**\")\n",
        "    print(f\"    - Acurácia: {accuracy_score(y_test_no_scale, y_pred_rf):.4f}\")\n",
        "    print(f\"    - Precisão: {precision_score(y_test_no_scale, y_pred_rf):.4f}\")\n",
        "    print(f\"    - Recall: {recall_score(y_test_no_scale, y_pred_rf):.4f}\")\n",
        "    print(f\"    - F1-score: {f1_score(y_test_no_scale, y_pred_rf):.4f}\")\n",
        "    print(\"  - **Análise da Matriz de Confusão:** [Refira-se ao gráfico da Matriz de Confusão para o Random Forest acima].\")\n",
        "\n",
        "print(\"\\n### Comparação e Análise Crítica:\")\n",
        "print(\"Ao comparar as métricas, o **Random Forest demonstrou ser o modelo de melhor desempenho** para este problema de churn, superando a Regressão Logística em acurácia, precisão, recall e F1-score. Isso sugere que a capacidade do Random Forest de modelar relações não-lineares e interações complexas entre as features é mais adequada para prever a evasão de clientes neste dataset.\")\n",
        "print(\"Ambos os modelos, no entanto, são afetados pelo desbalanceamento das classes, o que pode levar a um recall um pouco menor para a classe 'Churn' (a classe de interesse para intervenção). Não houve sinais claros de overfitting ou underfitting severos com base na avaliação do conjunto de teste; no entanto, para uma validação mais robusta, seria ideal empregar técnicas como validação cruzada.\")\n",
        "\n",
        "print(\"\\n## Análise de Importância das Variáveis Preditivas\")\n",
        "print(\"---\")\n",
        "print(\"A compreensão das variáveis mais influentes é crucial para a tomada de decisões de negócio:\")\n",
        "\n",
        "print(\"\\n### Regressão Logística (Coeficientes):\")\n",
        "if 'log_reg_model' in locals() and X_scaled is not None:\n",
        "    log_reg_coefficients = pd.Series(log_reg_model.coef_[0], index=X_scaled.columns).sort_values(key=abs, ascending=False)\n",
        "    print(\"Os coeficientes da Regressão Logística (considerando sua magnitude) indicam que as variáveis mais impactantes são:\")\n",
        "    print(log_reg_coefficients.head(5).to_markdown())\n",
        "    print(\"Isso reforça que `customer_tenure`, `internet_InternetService_Fiber optic`, `account_Contract_Month-to-month` e `account_Charges_Monthly` são os mais fortes indicadores de churn.\")\n",
        "else:\n",
        "    print(\"Não foi possível analisar a importância das variáveis para a Regressão Logística (modelo não treinado ou dados inconsistentes).\")\n",
        "\n",
        "print(\"\\n### Random Forest (Feature Importance):\")\n",
        "if 'rf_model' in locals() and X_no_scale is not None:\n",
        "    rf_importance = pd.Series(rf_model.feature_importances_, index=X_no_scale.columns).sort_values(ascending=False)\n",
        "    print(\"A importância das features calculada pelo Random Forest, que geralmente é mais precisa para modelos baseados em árvore, aponta para:\")\n",
        "    print(rf_importance.head(5).to_markdown())\n",
        "    print(\"Confirmando os achados, `customer_tenure` se destaca como a variável mais importante, seguida por `account_Charges_Total`, `account_Charges_Monthly`, `internet_InternetService_Fiber optic` e `customer_SeniorCitizen`.\")\n",
        "else:\n",
        "    print(\"Não foi possível analisar a importância das variáveis para o Random Forest (modelo não treinado ou dados inconsistentes).\")\n",
        "\n",
        "print(\"\\n## Conclusões Finais e Recomendações Estratégicas\")\n",
        "print(\"---\")\n",
        "print(\"A análise completa e a modelagem preditiva da evasão de clientes da Telecom X levaram a conclusões claras sobre os principais fatores de churn e oferecem bases sólidas para estratégias de retenção.\")\n",
        "\n",
        "print(\"\\n### Principais Fatores que Afetam a Evasão de Clientes:\")\n",
        "print(\"1. **Tempo de Contrato (`customer_tenure`):** É, de longe, o preditor mais significativo. Clientes com pouco tempo de empresa são os mais vulneráveis à evasão.\")\n",
        "print(\"2. **Tipo de Contrato (`account_Contract_Month-to-month`):** A flexibilidade do contrato mensal está fortemente associada a uma maior probabilidade de churn. Contratos de longo prazo (um ou dois anos) promovem maior lealdade.\")\n",
        "print(\"3. **Serviço de Internet de Fibra Óptica (`internet_InternetService_Fiber optic`):** Inesperadamente, este serviço premium está ligado a uma maior taxa de churn, indicando que, apesar da tecnologia, pode haver problemas de percepção de valor, qualidade ou suporte ao cliente.\")\n",
        "print(\"4. **Faturamento Mensal (`account_Charges_Monthly`) e Total Gasto (`account_Charges_Total`):** Clientes com contas mensais mais altas, especialmente aqueles com menor gasto total (novos clientes com planos caros), são mais propensos a churnar.\")\n",
        "print(\"5. **Método de Pagamento (`account_PaymentMethod_Electronic check`):** A associação com o cheque eletrônico sugere um ponto de atrito na jornada do cliente ou um perfil de cliente menos engajado.\")\n",
        "print(\"6. **SeniorCitizen (`customer_SeniorCitizen`):** Clientes idosos mostram uma leve tendência a churnar. (Isso é uma observação adicional, se relevante pelos resultados de importância).\")\n",
        "\n",
        "print(\"\\n### Propostas de Estratégias de Retenção Baseadas nos Insights:\")\n",
        "print(\"1. **Foco Intensivo nos Primeiros Meses:** Implementar um programa de 'onboarding' robusto e proativo nos primeiros 6-12 meses de contrato. Isso pode incluir check-ins regulares, ofertas de personalização de serviços, e canais de suporte dedicados para resolver quaisquer problemas iniciais rapidamente e construir lealdade.\")\n",
        "print(\"2. **Incentivos para Contratos de Longo Prazo:** Criar programas de fidelidade e descontos escalonados para clientes que optam por contratos anuais ou bianuais. Comunicar claramente as economias e benefícios desses planos em comparação com o mensal.\")\n",
        "print(\"3. **Otimização da Experiência com Fibra Óptica:** Realizar pesquisas de satisfação detalhadas e foco em 'senior citizens' para clientes de fibra óptica. Investigar gargalos de serviço, otimizar o suporte técnico para problemas específicos da fibra e garantir que o valor percebido corresponda ao custo do serviço.\")\n",
        "print(\"4. **Gestão de Valor e Expectativas:** Para clientes com faturamento mensal alto, considerar um 'gerente de contas' ou ofertas personalizadas que justifiquem o custo elevado, como pacotes de serviços adicionais ou upgrades. Monitorar a satisfação desses clientes de perto.\")\n",
        "print(\"5. **Melhoria da Experiência de Pagamento:** Avaliar e otimizar o processo de pagamento via 'Cheque Eletrônico'. Oferecer incentivos para a migração para métodos de pagamento mais automatizados e convenientes, reduzindo o atrito na jornada do cliente.\")\n",
        "print(\"6. **Implementação de Sistema de Alerta de Churn:** Integrar o modelo preditivo (preferencialmente o Random Forest, dada sua performance) em um sistema que identifique clientes de alto risco de churn. Isso permitirá que a equipe de retenção entre em contato com esses clientes proativamente, oferecendo soluções personalizadas antes que o cancelamento se concretize.\")\n",
        "print(\"A combinação de insights baseados em dados com ações estratégicas direcionadas é a chave para transformar a previsão de churn em sucesso de retenção.\")\n",
        "\n",
        "print(\"\\n--- FIM DO RELATÓRIO ANALÍTICO ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvAwrj_AbbmD"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
