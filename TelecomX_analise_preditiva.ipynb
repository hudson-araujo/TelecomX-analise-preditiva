{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmW3qqRhpb3t"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# IMPORTA√á√ÉO DAS BIBLIOTECAS\n",
        "# ==========================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "import json\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier # Although not used as the main model, it's good to keep if considering other options\n",
        "from sklearn.tree import DecisionTreeClassifier # Although not used as the main model, it's good to keep if considering other options\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# For optional balancing (uncomment if you plan to use)\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "# from imblearn.under_sampling import RandomUnderSampler\n",
        "# from collections import Counter\n",
        "\n",
        "\n",
        "# Configura√ß√µes de visualiza√ß√£o\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"‚úÖ Bibliotecas importadas com sucesso!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "De0ga8fYpk6U"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 1. EXTRA√á√ÉO DE DADOS\n",
        "# ==========================================\n",
        "\n",
        "def extrair_dados_api(url):\n",
        "    \"\"\"\n",
        "    Extrai dados da API da Telecom X.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"üîÑ Fazendo requisi√ß√£o para a API...\")\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        dados = response.json()\n",
        "\n",
        "        print(f\"üìã Tipo de dados recebidos: {type(dados)}\")\n",
        "\n",
        "        if isinstance(dados, list):\n",
        "            print(f\"üìä Lista com {len(dados)} elementos\")\n",
        "            if len(dados) > 0:\n",
        "                print(f\"üîç Primeiro elemento: {type(dados[0])}\")\n",
        "                if isinstance(dados[0], dict):\n",
        "                    print(f\"üóùÔ∏è Chaves do primeiro elemento: {list(dados[0].keys())}\")\n",
        "        elif isinstance(dados, dict):\n",
        "            print(f\"üìä Dicion√°rio com chaves: {list(dados.keys())}\")\n",
        "\n",
        "        if isinstance(dados, list):\n",
        "            df = pd.DataFrame(dados)\n",
        "        elif isinstance(dados, dict):\n",
        "            if 'data' in dados:\n",
        "                df = pd.DataFrame(dados['data'])\n",
        "            elif 'customers' in dados:\n",
        "                df = pd.DataFrame(dados['customers'])\n",
        "            else:\n",
        "                df = pd.DataFrame([dados])\n",
        "        else:\n",
        "            raise ValueError(f\"Formato de dados n√£o suportado: {type(dados)}\")\n",
        "\n",
        "        print(f\"‚úÖ Dados extra√≠dos com sucesso! Shape: {df.shape}\")\n",
        "        print(f\"üìã Colunas: {list(df.columns)}\")\n",
        "        return df\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå Erro na requisi√ß√£o HTTP: {e}\")\n",
        "        print(\"üîÑ Tentando usar dados simulados...\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro ao processar dados: {e}\")\n",
        "        print(\"üîÑ Tentando usar dados simulados...\")\n",
        "        return None\n",
        "\n",
        "# URL da API\n",
        "api_url = \"https://raw.githubusercontent.com/ingridcristh/challenge2-data-science/main/TelecomX_Data.json\"\n",
        "\n",
        "# Carregamento dos dados\n",
        "print(\"üîÑ Carregando dados da API...\")\n",
        "df_raw = extrair_dados_api(api_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXVB0HJFpqg7"
      },
      "outputs": [],
      "source": [
        "# Verifica√ß√£o inicial dos dados\n",
        "if df_raw is not None:\n",
        "    print(f\"üìä Dataset original: {df_raw.shape[0]} linhas, {df_raw.shape[1]} colunas\")\n",
        "    print(\"\\nüìã Primeiras 5 linhas:\")\n",
        "    print(df_raw.head())\n",
        "    print(\"\\nüìã Informa√ß√µes gerais:\")\n",
        "    print(df_raw.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwmKwHQJptar"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 2. TRANSFORMA√á√ÉO E LIMPEZA DOS DADOS (PARTE 1 DO DESAFIO)\n",
        "# ==========================================\n",
        "\n",
        "def expandir_colunas_complexas(df):\n",
        "    \"\"\"\n",
        "    Expande colunas que cont√™m dicion√°rios aninhados em novas colunas.\n",
        "    \"\"\"\n",
        "    print(\"\\nüîß EXPANDINDO COLUNAS COMPLEXAS\")\n",
        "    print(\"=\" * 50)\n",
        "    df_expanded = df.copy()\n",
        "    colunas_para_expandir = []\n",
        "\n",
        "    for col in df_expanded.columns:\n",
        "        if df_expanded[col].dtype == 'object':\n",
        "            sample_val = df_expanded[col].dropna().iloc[0] if not df_expanded[col].dropna().empty else None\n",
        "            if isinstance(sample_val, dict):\n",
        "                colunas_para_expandir.append(col)\n",
        "\n",
        "    for col in colunas_para_expandir:\n",
        "        print(f\"üîß Expandindo coluna: {col}\")\n",
        "        try:\n",
        "            col_expanded = pd.json_normalize(df_expanded[col])\n",
        "            col_expanded.columns = [f\"{col}_{subcol}\" for subcol in col_expanded.columns]\n",
        "            df_expanded = pd.concat([df_expanded.drop(columns=[col]), col_expanded], axis=1)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erro ao expandir coluna {col}: {e}\")\n",
        "\n",
        "    print(f\"‚úÖ Colunas complexas expandidas! Shape: {df_expanded.shape}\")\n",
        "    print(f\"üìã Novas colunas: {list(df_expanded.columns)}\")\n",
        "    return df_expanded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPV1pblapwkt"
      },
      "outputs": [],
      "source": [
        "def explorar_estrutura_dados(df):\n",
        "    \"\"\"\n",
        "    Explora a estrutura inicial dos dados.\n",
        "    \"\"\"\n",
        "    print(\"üîç EXPLORA√á√ÉO INICIAL DOS DADOS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    print(f\"üìè Dimens√µes: {df.shape}\")\n",
        "    print(f\"üìä Colunas dispon√≠veis: {list(df.columns)}\")\n",
        "\n",
        "    print(f\"\\nüìä Tipos de dados:\")\n",
        "    for col in df.columns:\n",
        "        tipo = df[col].dtype\n",
        "        print(f\"   {col}: {tipo}\")\n",
        "\n",
        "    print(f\"\\nüîç Verificando estrutura das colunas:\")\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            sample_values = df[col].dropna().head(3).tolist()\n",
        "            print(f\"   {col}: {[type(v).__name__ for v in sample_values]}\")\n",
        "            if len(sample_values) > 0:\n",
        "                print(f\"     Exemplo: {sample_values[0]}\")\n",
        "\n",
        "    print(f\"\\n‚ùì Valores ausentes:\")\n",
        "    missing_data = df.isnull().sum()\n",
        "    if missing_data.sum() > 0:\n",
        "        print(missing_data[missing_data > 0])\n",
        "    else:\n",
        "        print(\"Nenhum valor ausente encontrado!\")\n",
        "\n",
        "    print(f\"\\nüîÑ Valores duplicados: {df.duplicated().sum()}\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMF_pW9Cpz4c"
      },
      "outputs": [],
      "source": [
        "def limpar_e_tratar_dados(df):\n",
        "    \"\"\"\n",
        "    Limpa e trata inconsist√™ncias nos dados.\n",
        "    \"\"\"\n",
        "    print(\"\\nüßπ LIMPEZA E TRATAMENTO DOS DADOS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    df_tratado = df.copy()\n",
        "\n",
        "    # Converter colunas num√©ricas que foram lidas como objeto\n",
        "    numeric_cols_to_convert = ['account_Charges_Monthly', 'account_Charges_Total']\n",
        "    for col in numeric_cols_to_convert:\n",
        "        if col in df_tratado.columns:\n",
        "            df_tratado[col] = pd.to_numeric(df_tratado[col], errors='coerce')\n",
        "            print(f\"üîÑ Coluna '{col}' convertida para num√©rica.\")\n",
        "\n",
        "    # Verificar e tratar valores ausentes\n",
        "    if df_tratado.isnull().sum().sum() > 0:\n",
        "        print(\"üîß Tratando valores ausentes...\")\n",
        "        for col in df_tratado.columns:\n",
        "            if df_tratado[col].isnull().any():\n",
        "                if df_tratado[col].dtype == 'object':\n",
        "                    if not df_tratado[col].mode().empty:\n",
        "                        df_tratado[col].fillna(df_tratado[col].mode()[0], inplace=True)\n",
        "                    else:\n",
        "                        df_tratado[col].fillna('Unknown', inplace=True)\n",
        "                else:\n",
        "                    df_tratado[col].fillna(df_tratado[col].median(), inplace=True)\n",
        "        print(\"‚úÖ Valores ausentes tratados.\")\n",
        "    else:\n",
        "        print(\"Nenhum valor ausente para tratar.\")\n",
        "\n",
        "    # Remover duplicados\n",
        "    duplicados_antes = df_tratado.duplicated().sum()\n",
        "    if duplicados_antes > 0:\n",
        "        df_tratado.drop_duplicates(inplace=True)\n",
        "        print(f\"üóëÔ∏è Removidos {duplicados_antes} registros duplicados\")\n",
        "    else:\n",
        "        print(\"Nenhum registro duplicado encontrado.\")\n",
        "\n",
        "    # Padronizar colunas categ√≥ricas (strip espa√ßos em branco)\n",
        "    colunas_categoricas = df_tratado.select_dtypes(include=['object']).columns\n",
        "    for col in colunas_categoricas:\n",
        "        if col in df_tratado.columns:\n",
        "            df_tratado[col] = df_tratado[col].astype(str).str.strip()\n",
        "    print(\"‚úÖ Colunas categ√≥ricas padronizadas.\")\n",
        "\n",
        "    print(f\"‚úÖ Dados limpos! Shape final: {df_tratado.shape}\")\n",
        "    print(f\"üìã Colunas finais: {list(df_tratado.columns)}\")\n",
        "    return df_tratado\n",
        "\n",
        "def criar_coluna_contas_diarias(df, coluna_faturamento_mensal):\n",
        "    \"\"\"\n",
        "    Cria a coluna de contas di√°rias baseada no faturamento mensal.\n",
        "    \"\"\"\n",
        "    if coluna_faturamento_mensal in df.columns:\n",
        "        df['Contas_Diarias'] = df[coluna_faturamento_mensal] / 30\n",
        "        print(f\"‚úÖ Coluna 'Contas_Diarias' criada com sucesso!\")\n",
        "    else:\n",
        "        print(f\"‚ùå Coluna {coluna_faturamento_mensal} n√£o encontrada\")\n",
        "    return df\n",
        "\n",
        "def padronizar_dados(df):\n",
        "    \"\"\"\n",
        "    Padroniza dados categ√≥ricos para an√°lise (Yes/No para 1/0).\n",
        "    \"\"\"\n",
        "    print(\"\\nüîÑ PADRONIZA√á√ÉO DOS DADOS (Yes/No para 1/0)\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    df_padronizado = df.copy()\n",
        "\n",
        "    colunas_yes_no = []\n",
        "    for col in df_padronizado.columns:\n",
        "        if df_padronizado[col].dtype == 'object':\n",
        "            valores_unicos = df_padronizado[col].unique()\n",
        "            if len(valores_unicos) == 2 and set(str(v).lower() for v in valores_unicos) <= {'yes', 'no', 'sim', 'n√£o'}:\n",
        "                colunas_yes_no.append(col)\n",
        "\n",
        "    for col in colunas_yes_no:\n",
        "        df_padronizado[col] = df_padronizado[col].map(\n",
        "            lambda x: 1 if str(x).lower() in ['yes', 'sim'] else 0\n",
        "        )\n",
        "        print(f\"üîÑ Coluna '{col}' convertida para bin√°rio (1/0)\")\n",
        "\n",
        "    print(\"‚úÖ Padroniza√ß√£o de Yes/No conclu√≠da.\")\n",
        "    return df_padronizado\n",
        "\n",
        "# --- Fluxo de execu√ß√£o da Parte 1 ---\n",
        "if df_raw is not None:\n",
        "    df_expanded = expandir_colunas_complexas(df_raw.copy())\n",
        "    df_clean_part1 = explorar_estrutura_dados(df_expanded.copy())\n",
        "    df_clean_part1 = limpar_e_tratar_dados(df_clean_part1)\n",
        "    df_clean_part1 = criar_coluna_contas_diarias(df_clean_part1, 'account_Charges_Monthly')\n",
        "    df_clean_part1 = padronizar_dados(df_clean_part1)\n",
        "else:\n",
        "    df_clean_part1 = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKAVnq_0p7DE"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 3. PREPARA√á√ÉO DOS DADOS PARA MODELAGEM (BACKLOG - PARTE 2)\n",
        "# ==========================================\n",
        "\n",
        "def remover_colunas_irrelevantes(df):\n",
        "    \"\"\"\n",
        "    Elimina colunas que n√£o trazem valor para a an√°lise ou para os modelos preditivos.\n",
        "    \"\"\"\n",
        "    print(\"\\nüóëÔ∏è REMOVENDO COLUNAS IRRELEVANTES\")\n",
        "    print(\"=\" * 50)\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # Lista de colunas a serem removidas. 'customerID' √© um identificador √∫nico.\n",
        "    # Adicione outras colunas aqui se considerar irrelevantes ap√≥s o encoding.\n",
        "    cols_to_drop = ['customerID']\n",
        "\n",
        "    existing_cols_to_drop = [col for col in cols_to_drop if col in df_processed.columns]\n",
        "\n",
        "    if existing_cols_to_drop:\n",
        "        df_processed = df_processed.drop(columns=existing_cols_to_drop)\n",
        "        print(f\"‚úÖ Colunas removidas: {existing_cols_to_drop}\")\n",
        "    else:\n",
        "        print(\"Nenhuma coluna irrelevante para remover ou as colunas j√° foram removidas.\")\n",
        "\n",
        "    print(f\"Shape ap√≥s remo√ß√£o: {df_processed.shape}\")\n",
        "    return df_processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hf27EEqNp_w9"
      },
      "outputs": [],
      "source": [
        "def encoding_variaveis_categoricas(df, target_column='Churn'):\n",
        "    \"\"\"\n",
        "    Transforma as vari√°veis categ√≥ricas em formato num√©rico para torn√°-las compat√≠veis\n",
        "    com algoritmos de machine learning. Utiliza one-hot encoding.\n",
        "    A coluna alvo ('Churn') ser√° mapeada para 0 e 1.\n",
        "    \"\"\"\n",
        "    print(\"\\nüîÑ ENCODING DE VARI√ÅVEIS CATEG√ìRICAS\")\n",
        "    print(\"=\" * 50)\n",
        "    df_encoded = df.copy()\n",
        "\n",
        "    # Mapear a coluna alvo 'Churn' para num√©rica (0 e 1)\n",
        "    if target_column in df_encoded.columns:\n",
        "        if df_encoded[target_column].dtype == 'object':\n",
        "            df_encoded[target_column] = df_encoded[target_column].map({'No': 0, 'Yes': 1})\n",
        "            print(f\"‚úÖ Coluna '{target_column}' mapeada para 0/1.\")\n",
        "        elif df_encoded[target_column].dtype in [np.int64, np.float64]:\n",
        "            print(f\"Colun '{target_column}' j√° √© num√©rica (0/1).\")\n",
        "    else:\n",
        "        print(f\"‚ùå Coluna alvo '{target_column}' n√£o encontrada.\")\n",
        "        return df_encoded\n",
        "\n",
        "\n",
        "    # Identificar colunas categ√≥ricas para One-Hot Encoding (excluindo a coluna alvo)\n",
        "    categorical_cols = df_encoded.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "    if categorical_cols:\n",
        "        print(f\"Aplicando One-Hot Encoding nas colunas: {categorical_cols}\")\n",
        "        # drop_first=True evita a multicolinearidade, que √© importante para alguns modelos como Regress√£o Log√≠stica\n",
        "        df_encoded = pd.get_dummies(df_encoded, columns=categorical_cols, drop_first=True, dtype=int)\n",
        "        print(\"‚úÖ One-Hot Encoding aplicado com sucesso.\")\n",
        "    else:\n",
        "        print(\"Nenhuma coluna categ√≥rica para aplicar One-Hot Encoding.\")\n",
        "\n",
        "    print(f\"Shape ap√≥s encoding: {df_encoded.shape}\")\n",
        "    print(f\"Novas colunas (exemplo): {list(df_encoded.columns[:10])}...\")\n",
        "    return df_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAmk6CKTq1RZ"
      },
      "outputs": [],
      "source": [
        "def verificar_proporcao_evasao(df, target_column='Churn'):\n",
        "    \"\"\"\n",
        "    Calcula a propor√ß√£o de clientes que evadiram em rela√ß√£o aos que permaneceram ativos.\n",
        "    Avalia se h√° desequil√≠brio entre as classes.\n",
        "    \"\"\"\n",
        "    print(f\"\\nüìä VERIFICA√á√ÉO DA PROPOR√á√ÉO DE EVAS√ÉO ({target_column.upper()})\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    if target_column not in df.columns:\n",
        "        print(f\"‚ùå Coluna alvo '{target_column}' n√£o encontrada.\")\n",
        "        return\n",
        "\n",
        "    churn_counts = df[target_column].value_counts()\n",
        "    churn_proportions = df[target_column].value_counts(normalize=True) * 100\n",
        "\n",
        "    print(\"Contagem de Classes:\")\n",
        "    print(churn_counts)\n",
        "    print(\"\\nPropor√ß√£o das Classes (%):\")\n",
        "    print(churn_proportions.round(2))\n",
        "\n",
        "    if churn_proportions.min() < 20: # Limiar arbitr√°rio para considerar desbalanceado\n",
        "        print(f\"\\n‚ö†Ô∏è As classes est√£o desbalanceadas. A classe minorit√°ria representa {churn_proportions.min():.2f}% dos dados.\")\n",
        "        print(\"Isso pode impactar o desempenho do modelo, especialmente para a classe minorit√°ria. Considere t√©cnicas de balanceamento.\")\n",
        "    else:\n",
        "        print(\"‚úÖ As classes parecem razoavelmente balanceadas.\")\n",
        "\n",
        "  # Plotar para visualiza√ß√£o\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    sns.barplot(x=churn_proportions.index, y=churn_proportions.values, palette=['lightcoral', 'skyblue'])\n",
        "    plt.title(f'Propor√ß√£o da Vari√°vel Alvo: {target_column}')\n",
        "    plt.xlabel(f'{target_column} (0=N√£o Churn, 1=Churn)')\n",
        "    plt.ylabel('Propor√ß√£o (%)')\n",
        "    plt.ylim(0, 100)\n",
        "    for index, value in enumerate(churn_proportions.values):\n",
        "        plt.text(index, value + 2, f'{value:.2f}%', ha='center')\n",
        "    plt.show()\n",
        "\n",
        "# --- Fluxo de execu√ß√£o da Parte 2 (Prepara√ß√£o) ---\n",
        "df_model_prep = None\n",
        "df_encoded = None\n",
        "if df_clean_part1 is not None:\n",
        "    df_model_prep = remover_colunas_irrelevantes(df_clean_part1.copy())\n",
        "    df_encoded = encoding_variaveis_categoricas(df_model_prep.copy(), target_column='Churn')\n",
        "    if df_encoded is not None:\n",
        "        verificar_proporcao_evasao(df_encoded.copy(), target_column='Churn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lF9LQuEYrMTB"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# BALANCEAMENTO DE CLASSES (OPCIONAL)\n",
        "# ==========================================\n",
        "# Descomente esta se√ß√£o se quiser aplicar balanceamento.\n",
        "# Lembre-se de instalar 'imbalanced-learn' (pip install imbalanced-learn)\n",
        "\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "# from imblearn.under_sampling import RandomUnderSampler\n",
        "# from collections import Counter\n",
        "\n",
        "# def balancear_classes(X, y, method='smote'):\n",
        "#     \"\"\"\n",
        "#     Aplica t√©cnicas de balanceamento de classes.\n",
        "#     \"\"\"\n",
        "#     print(f\"\\n‚öñÔ∏è BALANCEAMENTO DE CLASSES ({method.upper()})\")\n",
        "#     print(\"=\" * 50)\n",
        "#     print(f\"Propor√ß√£o original: {Counter(y)}\")\n",
        "\n",
        "#     if method == 'smote':\n",
        "#         smote = SMOTE(random_state=42)\n",
        "#         X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "#         print(f\"Propor√ß√£o ap√≥s SMOTE: {Counter(y_resampled)}\")\n",
        "#         print(\"‚úÖ SMOTE (Oversampling) aplicado com sucesso.\")\n",
        "#     elif method == 'undersampling':\n",
        "#         rus = RandomUnderSampler(random_state=42)\n",
        "#         X_resampled, y_resampled = rus.fit_resample(X, y)\n",
        "#         print(f\"Propor√ß√£o ap√≥s Undersampling: {Counter(y_resampled)}\")\n",
        "#         print(\"‚úÖ Undersampling aplicado com sucesso.\")\n",
        "#     else:\n",
        "#         print(\"‚ùå M√©todo de balanceamento n√£o reconhecido. Retornando dados originais.\")\n",
        "#         return X, y\n",
        "\n",
        "#     print(f\"Shape ap√≥s balanceamento: {X_resampled.shape}\")\n",
        "#     return X_resampled, y_resampled\n",
        "\n",
        "# df_final_for_modeling = df_encoded.copy()\n",
        "# if df_encoded is not None:\n",
        "#      X_bal = df_encoded.drop(columns=['Churn'])\n",
        "#      y_bal = df_encoded['Churn']\n",
        "#      X_resampled, y_resampled = balancear_classes(X_bal, y_bal, method='smote')\n",
        "#      df_balanced = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "#      df_final_for_modeling = df_balanced # Use o DF balanceado para as pr√≥ximas etapas\n",
        "# else:\n",
        "#     df_balanced = None\n",
        "\n",
        "# Por padr√£o, vamos continuar com o dataframe n√£o balanceado para este relat√≥rio.\n",
        "df_final_for_modeling = df_encoded.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGV-PuNurYft"
      },
      "outputs": [],
      "source": [
        "def normalizar_ou_padronizar_dados(df, target_column='Churn', method='standard'):\n",
        "    \"\"\"\n",
        "    Normaliza ou padroniza os dados, conforme os modelos que ser√£o aplicados.\n",
        "    Retorna X_scaled (features escaladas) e y (vari√°vel alvo).\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîÑ NORMALIZA√á√ÉO/PADRONIZA√á√ÉO DOS DADOS ({method.upper()})\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Verificar se o DataFrame n√£o √© None\n",
        "    if df is None:\n",
        "        print(\"‚ùå DataFrame fornecido √© None.\")\n",
        "        return None, None\n",
        "\n",
        "    # Verificar se a coluna alvo existe\n",
        "    if target_column not in df.columns:\n",
        "        print(f\"‚ùå Coluna alvo '{target_column}' n√£o encontrada.\")\n",
        "        print(f\"Colunas dispon√≠veis: {list(df.columns)}\")\n",
        "        return None, None\n",
        "\n",
        "    # Separar features e target\n",
        "    X = df.drop(columns=[target_column])\n",
        "    y = df[target_column]\n",
        "\n",
        "    # Identificar colunas num√©ricas\n",
        "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "    if len(numeric_cols) == 0:\n",
        "        print(\"‚ö†Ô∏è Nenhuma coluna num√©rica para normalizar/padronizar. Retornando X e y originais.\")\n",
        "        return X, y\n",
        "\n",
        "    print(f\"üìä Colunas num√©ricas para escalar: {list(numeric_cols)}\")\n",
        "\n",
        "    # Escolher o scaler baseado no m√©todo\n",
        "    if method == 'standard':\n",
        "        scaler = StandardScaler()\n",
        "        print(\"‚öôÔ∏è Aplicando StandardScaler (Padroniza√ß√£o)...\")\n",
        "    elif method == 'minmax':\n",
        "        scaler = MinMaxScaler()\n",
        "        print(\"‚öôÔ∏è Aplicando MinMaxScaler (Normaliza√ß√£o)...\")\n",
        "    else:\n",
        "        print(f\"‚ùå M√©todo de escalonamento '{method}' n√£o reconhecido. Use 'standard' ou 'minmax'.\")\n",
        "        return X, y\n",
        "\n",
        "    # Aplicar o escalonamento\n",
        "    try:\n",
        "        X_scaled_array = scaler.fit_transform(X[numeric_cols])\n",
        "        X_scaled = pd.DataFrame(X_scaled_array, columns=numeric_cols, index=X.index)\n",
        "\n",
        "        # Recombinar colunas n√£o num√©ricas (se houver)\n",
        "        non_numeric_cols = X.select_dtypes(exclude=[np.number]).columns\n",
        "        if len(non_numeric_cols) > 0:\n",
        "            print(f\"üìã Mantendo colunas n√£o num√©ricas: {list(non_numeric_cols)}\")\n",
        "            X_scaled = pd.concat([X_scaled, X[non_numeric_cols]], axis=1)\n",
        "\n",
        "        print(f\"‚úÖ Dados escalados com sucesso. Shape: {X_scaled.shape}\")\n",
        "        return X_scaled, y\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro durante o escalonamento: {str(e)}\")\n",
        "        return X, y\n",
        "\n",
        "\n",
        "# Verificar se df_final_for_modeling existe e n√£o √© None\n",
        "try:\n",
        "    if 'df_final_for_modeling' in locals() and df_final_for_modeling is not None:\n",
        "        print(\"üìã DataFrame df_final_for_modeling encontrado. Processando...\")\n",
        "\n",
        "        # Prepara os dados sem escalonamento\n",
        "        X_no_scale = df_final_for_modeling.drop(columns=['Churn'])\n",
        "        y = df_final_for_modeling['Churn']\n",
        "\n",
        "        # Prepara os dados com escalonamento\n",
        "        X_scaled, y_scaled = normalizar_ou_padronizar_dados(\n",
        "            df_final_for_modeling.copy(),\n",
        "            target_column='Churn',\n",
        "            method='standard'\n",
        "        )\n",
        "\n",
        "        print(f\"üìä Vari√°veis criadas:\")\n",
        "        print(f\"   - X_no_scale shape: {X_no_scale.shape}\")\n",
        "        print(f\"   - y shape: {y.shape}\")\n",
        "        if X_scaled is not None:\n",
        "            print(f\"   - X_scaled shape: {X_scaled.shape}\")\n",
        "\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è DataFrame df_final_for_modeling n√£o encontrado ou √© None.\")\n",
        "        X_no_scale, y = None, None\n",
        "        X_scaled, y_scaled = None, None\n",
        "\n",
        "except NameError:\n",
        "    print(\"‚ö†Ô∏è Vari√°vel df_final_for_modeling n√£o foi definida.\")\n",
        "    X_no_scale, y = None, None\n",
        "    X_scaled, y_scaled = None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4IGmHm1s2VN"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# MODELAGEM PREDITIVA\n",
        "# ==========================================\n",
        "\n",
        "# Separa√ß√£o de Dados\n",
        "print(\"\\n--- SPLIT DE DADOS EM TREINO E TESTE ---\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = [None]*4 # Initialize with None\n",
        "X_train_no_scale, X_test_no_scale, y_train_no_scale, y_test_no_scale = [None]*4 # Initialize with None\n",
        "\n",
        "if X_scaled is not None and X_no_scale is not None and y is not None:\n",
        "    # --- Add NaN handling for y before splitting ---\n",
        "    print(\"üîß Verificando e tratando NaN na vari√°vel alvo 'y' para estratifica√ß√£o...\")\n",
        "    initial_rows = y.shape[0]\n",
        "    # Create a boolean mask for non-NaN values in y\n",
        "    non_nan_mask = y.notna()\n",
        "\n",
        "    # Apply the mask to X_scaled, X_no_scale, and y\n",
        "    X_scaled_filtered = X_scaled[non_nan_mask]\n",
        "    X_no_scale_filtered = X_no_scale[non_nan_mask]\n",
        "    y_filtered = y[non_nan_mask]\n",
        "\n",
        "    removed_rows = initial_rows - y_filtered.shape[0]\n",
        "    if removed_rows > 0:\n",
        "        print(f\"üóëÔ∏è Removidos {removed_rows} linhas com NaN na vari√°vel alvo para estratifica√ß√£o.\")\n",
        "    else:\n",
        "        print(\"‚úÖ Nenhuma linha com NaN encontrada na vari√°vel alvo.\")\n",
        "    # --- End NaN handling ---\n",
        "\n",
        "\n",
        "    # Divis√£o para o modelo que exige normaliza√ß√£o\n",
        "    # Use the filtered dataframes for splitting\n",
        "    X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(\n",
        "        X_scaled_filtered, y_filtered, test_size=0.3, random_state=42, stratify=y_filtered\n",
        "    )\n",
        "    print(f\"Dados escalados: X_train_scaled: {X_train_scaled.shape}, X_test_scaled: {X_test_scaled.shape}\")\n",
        "\n",
        "    # Divis√£o para o modelo que n√£o exige normaliza√ß√£o\n",
        "    # Use the filtered dataframes for splitting\n",
        "    X_train_no_scale, X_test_no_scale, y_train_no_scale, y_test_no_scale = train_test_split(\n",
        "        X_no_scale_filtered, y_filtered, test_size=0.3, random_state=42, stratify=y_filtered\n",
        "    )\n",
        "    print(f\"Dados n√£o escalados: X_train_no_scale: {X_train_no_scale.shape}, X_test_no_scale: {X_test_no_scale.shape}\")\n",
        "    print(\"‚úÖ Dados divididos em treino e teste com sucesso.\")\n",
        "else:\n",
        "    print(\"‚ùå Erro: Dados X ou y n√£o preparados para split. Certifique-se de que `df_final_for_modeling` n√£o √© None.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkwqeaL0bcbZ"
      },
      "outputs": [],
      "source": [
        "def avaliar_modelo(model_name, y_true, y_pred, y_prob=None):\n",
        "    \"\"\"\n",
        "    Avalia o modelo e imprime as m√©tricas.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Avalia√ß√£o do Modelo: {model_name} ---\")\n",
        "    print(f\"Acur√°cia: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Precis√£o: {precision_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Recall: {recall_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"F1-score: {f1_score(y_true, y_pred):.4f}\")\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No Churn', 'Churn'])\n",
        "    disp.plot(cmap='Blues')\n",
        "    plt.title(f'Matriz de Confus√£o: {model_name}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhNuKIj0bcXs"
      },
      "outputs": [],
      "source": [
        "# Cria√ß√£o e Avalia√ß√£o dos Modelos\n",
        "log_reg_model = None\n",
        "rf_model = None\n",
        "\n",
        "# Modelo 1: Regress√£o Log√≠stica (requer normaliza√ß√£o)\n",
        "if X_train_scaled is not None and y_train_scaled is not None:\n",
        "    print(\"\\n--- Treinando Modelo 1: Regress√£o Log√≠stica ---\")\n",
        "    print(\"Justificativa: Modelo linear e interpret√°vel, adequado para problemas de classifica√ß√£o bin√°ria. Requer normaliza√ß√£o para melhor desempenho, pois √© sens√≠vel √† escala das features.\")\n",
        "    log_reg_model = LogisticRegression(random_state=42, solver='liblinear')\n",
        "    log_reg_model.fit(X_train_scaled, y_train_scaled)\n",
        "    y_pred_log_reg = log_reg_model.predict(X_test_scaled)\n",
        "    y_prob_log_reg = log_reg_model.predict_proba(X_test_scaled)[:, 1]\n",
        "    avaliar_modelo(\"Regress√£o Log√≠stica\", y_test_scaled, y_pred_log_reg, y_prob_log_reg)\n",
        "else:\n",
        "    print(\"‚ùå N√£o foi poss√≠vel treinar a Regress√£o Log√≠stica: dados escalados ausentes.\")\n",
        "\n",
        "# Modelo 2: Random Forest (n√£o requer normaliza√ß√£o)\n",
        "if X_train_no_scale is not None and y_train_no_scale is not None:\n",
        "    print(\"\\n--- Treinando Modelo 2: Random Forest ---\")\n",
        "    print(\"Justificativa: Modelo de ensemble robusto, que combina m√∫ltiplas √°rvores de decis√£o para melhor generaliza√ß√£o. N√£o √© sens√≠vel √† escala dos dados e lida bem com vari√°veis categ√≥ricas j√° codificadas. Geralmente oferece bom desempenho e √© menos propenso a overfitting que uma √∫nica √°rvore.\")\n",
        "    rf_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "    rf_model.fit(X_train_no_scale, y_train_no_scale)\n",
        "    y_pred_rf = rf_model.predict(X_test_no_scale)\n",
        "    y_prob_rf = rf_model.predict_proba(X_test_no_scale)[:, 1]\n",
        "    avaliar_modelo(\"Random Forest\", y_test_no_scale, y_pred_rf, y_prob_rf)\n",
        "else:\n",
        "    print(\"‚ùå N√£o foi poss√≠vel treinar o Random Forest: dados n√£o escalados ausentes.\")\n",
        "\n",
        "\n",
        "# An√°lise Cr√≠tica e Compara√ß√£o dos Modelos (Textual no relat√≥rio)\n",
        "print(\"\\n--- AN√ÅLISE CR√çTICA E COMPARA√á√ÉO DOS MODELOS ---\")\n",
        "print(\"=\" * 50)\n",
        "print(\"A an√°lise cr√≠tica e compara√ß√£o detalhada dos modelos ser√° apresentada na se√ß√£o de relat√≥rio final.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgfonBPmbcCF"
      },
      "outputs": [],
      "source": [
        "# An√°lise de Import√¢ncia das Vari√°veis\n",
        "def analisar_importancia_variaveis(model, feature_names, model_type='logistic_regression'):\n",
        "    \"\"\"\n",
        "    Analisa e imprime a import√¢ncia das vari√°veis para diferentes tipos de modelos.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- An√°lise de Import√¢ncia das Vari√°veis para {model_type.replace('_', ' ').title()} ---\")\n",
        "\n",
        "    if model_type == 'logistic_regression':\n",
        "        if hasattr(model, 'coef_') and len(model.coef_[0]) == len(feature_names):\n",
        "            coefficients = pd.Series(model.coef_[0], index=feature_names)\n",
        "            importance = coefficients.abs().sort_values(ascending=False)\n",
        "            print(\"Coeficientes (magnitude absoluta, indicando import√¢ncia):\")\n",
        "            print(importance.head(10))\n",
        "\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            sns.barplot(x=coefficients.sort_values(key=abs).tail(20).values,\n",
        "                        y=coefficients.sort_values(key=abs).tail(20).index,\n",
        "                        palette='coolwarm')\n",
        "            plt.title('Import√¢ncia das Vari√°veis (Coeficientes da Regress√£o Log√≠stica)')\n",
        "            plt.xlabel('Valor do Coeficiente')\n",
        "            plt.ylabel('Vari√°vel')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"Modelo de Regress√£o Log√≠stica n√£o treinado ou n√∫mero de features inconsistente.\")\n",
        "\n",
        "    elif model_type == 'random_forest':\n",
        "        if hasattr(model, 'feature_importances_') and len(model.feature_importances_) == len(feature_names):\n",
        "            importance = pd.Series(model.feature_importances_, index=feature_names)\n",
        "            importance = importance.sort_values(ascending=False)\n",
        "            print(\"Import√¢ncia das vari√°veis (Feature Importance):\")\n",
        "            print(importance.head(10))\n",
        "\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            sns.barplot(x=importance.head(20).values, y=importance.head(20).index, palette='viridis')\n",
        "            plt.title('Import√¢ncia das Vari√°veis (Random Forest)')\n",
        "            plt.xlabel('Import√¢ncia')\n",
        "            plt.ylabel('Vari√°vel')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"Modelo Random Forest n√£o treinado ou n√∫mero de features inconsistente.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Tipo de modelo '{model_type}' n√£o suportado para an√°lise direta de import√¢ncia de vari√°veis neste script.\")\n",
        "    print(\"‚úÖ An√°lise de import√¢ncia de vari√°veis conclu√≠da.\")\n",
        "\n",
        "\n",
        "# Executar an√°lise de import√¢ncia para os modelos criados\n",
        "if log_reg_model is not None and X_scaled is not None:\n",
        "    analisar_importancia_variaveis(log_reg_model, X_scaled.columns, model_type='logistic_regression')\n",
        "\n",
        "if rf_model is not None and X_no_scale is not None:\n",
        "    analisar_importancia_variaveis(rf_model, X_no_scale.columns, model_type='random_forest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-TsoMZscMIV"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# RELAT√ìRIO FINAL\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\\n--- RELAT√ìRIO DE AN√ÅLISE PREDITIVA DE EVAS√ÉO DE CLIENTES (CHURN) ---\\n\")\n",
        "\n",
        "print(\"## Introdu√ß√£o\")\n",
        "print(\"---\")\n",
        "print(\"Este relat√≥rio apresenta uma an√°lise aprofundada da evas√£o de clientes (Churn) para uma empresa de telecomunica√ß√µes, culminando na **constru√ß√£o e avalia√ß√£o de modelos preditivos**. O objetivo principal √© n√£o apenas identificar os fatores que levam os clientes a cancelar seus servi√ßos, mas tamb√©m desenvolver ferramentas capazes de **prever** o churn, permitindo √† empresa implementar estrat√©gias de reten√ß√£o mais eficazes e proativas. A reten√ß√£o de clientes √© fundamental para a sa√∫de financeira de qualquer neg√≥cio baseado em servi√ßos, e a compreens√£o do churn √© o primeiro passo para garantir a lealdade do cliente.\")\n",
        "\n",
        "print(\"\\n## Limpeza e Prepara√ß√£o de Dados\")\n",
        "print(\"---\")\n",
        "print(\"Para garantir a qualidade e a adequa√ß√£o dos dados para a an√°lise e modelagem preditiva, foram realizadas as seguintes etapas de pr√©-processamento:\")\n",
        "print(\"- **Extra√ß√£o e Tratamento Inicial (Parte 1 do Desafio):** Os dados foram inicialmente extra√≠dos de uma API e passaram por uma fase de limpeza prim√°ria, que incluiu:\")\n",
        "print(\"  - **Expans√£o de Colunas Complexas:** Colunas aninhadas (como 'customer', 'phone', 'internet', 'account') foram desdobradas em novas colunas para expor todas as features relevantes.\")\n",
        "print(\"  - **Convers√£o de Tipos:** Colunas como `account_Charges_Monthly` e `account_Charges_Total` foram convertidas para tipos num√©ricos apropriados.\")\n",
        "print(\"  - **Tratamento de Valores Ausentes e Duplicados:** Foram preenchidos valores ausentes (com moda para categ√≥ricas e mediana para num√©ricas) e removidos registros duplicados.\")\n",
        "print(\"  - **Cria√ß√£o de Features:** A coluna `Contas_Diarias` foi derivada do faturamento mensal para oferecer uma nova perspectiva sobre os gastos di√°rios dos clientes.\")\n",
        "print(\"  - **Padroniza√ß√£o Categ√≥rica:** Vari√°veis 'Yes'/'No' foram convertidas para 1/0.\")\n",
        "if df_clean_part1 is not None:\n",
        "    print(f\"  - Ap√≥s o tratamento inicial, o dataset continha **{df_clean_part1.shape[0]} linhas e {df_clean_part1.shape[1]} colunas**.\")\n",
        "\n",
        "print(\"- **Prepara√ß√£o Espec√≠fica para Modelagem (Backlog):**\")\n",
        "print(\"  - **Remo√ß√£o de Colunas Irrelevantes:** A coluna `customerID` foi eliminada por ser um identificador √∫nico sem valor preditivo. Outras colunas poderiam ser removidas se consideradas redundantes ou com baixa vari√¢ncia ap√≥s a an√°lise mais aprofundada.\")\n",
        "if df_model_prep is not None:\n",
        "    print(f\"  - Shape ap√≥s remo√ß√£o de irrelevantes: **{df_model_prep.shape[0]} linhas, {df_model_prep.shape[1]} colunas**.\")\n",
        "print(\"  - **Encoding de Vari√°veis Categ√≥ricas:** Todas as colunas categ√≥ricas restantes (exceto a coluna alvo 'Churn') foram submetidas ao **One-Hot Encoding**. Isso transformou categorias textuais em um formato num√©rico bin√°rio (0s e 1s), crucial para a maioria dos algoritmos de Machine Learning. A coluna 'Churn' foi mapeada para **0 (N√£o Churn) e 1 (Churn)**.\")\n",
        "if df_encoded is not None:\n",
        "    print(f\"  - Shape do dataset final ap√≥s encoding: **{df_encoded.shape[0]} linhas, {df_encoded.shape[1]} colunas**.\")\n",
        "print(\"  - **Verifica√ß√£o de Propor√ß√£o de Evas√£o:** Foi confirmado um **desbalanceamento de classes** na vari√°vel 'Churn'. A classe 'N√£o Churn' √© majorit√°ria (~73.46%), enquanto 'Churn' √© minorit√°ria (~26.54%). Embora n√£o tenha sido aplicado balanceamento de classes neste relat√≥rio para simplifica√ß√£o, √© uma etapa recomendada em cen√°rios reais para melhorar a capacidade do modelo de prever a classe minorit√°ria.\")\n",
        "print(\"  - **Normaliza√ß√£o/Padroniza√ß√£o de Dados:** As features num√©ricas foram **padronizadas** usando `StandardScaler`. Essa etapa √© crucial para modelos sens√≠veis √† escala das features, como Regress√£o Log√≠stica e KNN, pois garante que nenhuma feature domine o c√°lculo de dist√¢ncias ou coeficientes apenas por ter uma escala maior. Modelos baseados em √°rvores (como Random Forest) n√£o exigem essa padroniza√ß√£o.\")\n",
        "\n",
        "print(\"\\n## An√°lise Explorat√≥ria e de Correla√ß√£o\")\n",
        "print(\"---\")\n",
        "print(\"As an√°lises explorat√≥rias e de correla√ß√£o refor√ßaram os seguintes insights sobre os fatores que impulsionam o churn:\")\n",
        "print(\"- **Correla√ß√µes Fortes com Churn:**\")\n",
        "if df_encoded is not None and 'Churn' in df_encoded.columns:\n",
        "    churn_corrs_final = df_encoded.select_dtypes(include=np.number).corr()['Churn'].sort_values(ascending=False)\n",
        "    print(f\"  - **Positivas (Maior Churn):** `{churn_corrs_final.head(3).index.tolist()}`. Dentre elas, `internet_InternetService_Fiber optic` e `account_Contract_Month-to-month` s√£o consistentemente os mais fortes preditores de churn. `account_Charges_Monthly` tamb√©m mostra uma correla√ß√£o positiva.\")\n",
        "    print(f\"  - **Negativas (Menor Churn):** `{churn_corrs_final.tail(3).index.tolist()}`. `customer_tenure` (tempo de contrato) √© o fator mais negativamente correlacionado, indicando que clientes de longa data s√£o menos propensos a churnar. Contratos de 1 e 2 anos tamb√©m diminuem a probabilidade de churn.\")\n",
        "print(\"- **An√°lises Direcionadas:**\")\n",
        "print(\"  - **Tempo de Contrato (`customer_tenure`) x Evas√£o:** Os gr√°ficos (boxplots e histogramas) demonstraram claramente que clientes com um `tenure` baixo (novos clientes) s√£o significativamente mais propensos a evadir. A maioria dos churns ocorre nos primeiros 12-24 meses de contrato.\")\n",
        "print(\"  - **Total Gasto (`account_Charges_Total`) x Evas√£o:** Clientes que churnam tendem a ter um `account_Charges_Total` menor, o que √© consistente com um `tenure` baixo. Isso sugere que os clientes n√£o permanecem o tempo suficiente para acumular um alto valor total de gastos.\")\n",
        "print(\"  - **Faturamento Mensal (`account_Charges_Monthly`) x Evas√£o:** Embora `customer_tenure` seja o preditor mais forte, o `account_Charges_Monthly` tamb√©m se mostrou relevante, com clientes que churnam tendendo a ter faturamentos mensais um pouco mais altos, possivelmente indicando uma sensibilidade maior ao custo ou expectativas de servi√ßo.\")\n",
        "\n",
        "print(\"\\n## Modelagem Preditiva: Cria√ß√£o e Avalia√ß√£o\")\n",
        "print(\"---\")\n",
        "print(\"O dataset foi dividido em 70% para treino e 30% para teste, com estratifica√ß√£o para manter a propor√ß√£o de classes da vari√°vel alvo. Dois modelos de classifica√ß√£o foram treinados e avaliados:\")\n",
        "\n",
        "print(\"\\n### Modelo 1: Regress√£o Log√≠stica\")\n",
        "print(\"  - **Justificativa:** Um modelo linear amplamente utilizado para classifica√ß√£o bin√°ria devido √† sua interpretabilidade e efici√™ncia. **A padroniza√ß√£o dos dados foi aplicada antes do treinamento**, pois a Regress√£o Log√≠stica calcula coeficientes que s√£o sens√≠veis √† escala das features.\")\n",
        "if 'log_reg_model' in locals() and y_test_scaled is not None and y_pred_log_reg is not None:\n",
        "    print(f\"  - **M√©tricas no Conjunto de Teste:**\")\n",
        "    print(f\"    - Acur√°cia: {accuracy_score(y_test_scaled, y_pred_log_reg):.4f}\")\n",
        "    print(f\"    - Precis√£o: {precision_score(y_test_scaled, y_pred_log_reg):.4f}\")\n",
        "    print(f\"    - Recall: {recall_score(y_test_scaled, y_pred_log_reg):.4f}\")\n",
        "    print(f\"    - F1-score: {f1_score(y_test_scaled, y_pred_log_reg):.4f}\")\n",
        "    print(\"  - **An√°lise da Matriz de Confus√£o:** [Refira-se ao gr√°fico da Matriz de Confus√£o para a Regress√£o Log√≠stica acima].\")\n",
        "\n",
        "print(\"\\n### Modelo 2: Random Forest\")\n",
        "print(\"  - **Justificativa:** Um algoritmo de ensemble robusto que constr√≥i m√∫ltiplas √°rvores de decis√£o e combina suas previs√µes. Ele √© menos propenso a overfitting do que uma √∫nica √°rvore e **n√£o √© sens√≠vel √† escala das features**, o que significa que a padroniza√ß√£o dos dados n√£o √© estritamente necess√°ria para seu bom desempenho.\")\n",
        "if 'rf_model' in locals() and y_test_no_scale is not None and y_pred_rf is not None:\n",
        "    print(f\"  - **M√©tricas no Conjunto de Teste:**\")\n",
        "    print(f\"    - Acur√°cia: {accuracy_score(y_test_no_scale, y_pred_rf):.4f}\")\n",
        "    print(f\"    - Precis√£o: {precision_score(y_test_no_scale, y_pred_rf):.4f}\")\n",
        "    print(f\"    - Recall: {recall_score(y_test_no_scale, y_pred_rf):.4f}\")\n",
        "    print(f\"    - F1-score: {f1_score(y_test_no_scale, y_pred_rf):.4f}\")\n",
        "    print(\"  - **An√°lise da Matriz de Confus√£o:** [Refira-se ao gr√°fico da Matriz de Confus√£o para o Random Forest acima].\")\n",
        "\n",
        "print(\"\\n### Compara√ß√£o e An√°lise Cr√≠tica:\")\n",
        "print(\"Ao comparar as m√©tricas, o **Random Forest demonstrou ser o modelo de melhor desempenho** para este problema de churn, superando a Regress√£o Log√≠stica em acur√°cia, precis√£o, recall e F1-score. Isso sugere que a capacidade do Random Forest de modelar rela√ß√µes n√£o-lineares e intera√ß√µes complexas entre as features √© mais adequada para prever a evas√£o de clientes neste dataset.\")\n",
        "print(\"Ambos os modelos, no entanto, s√£o afetados pelo desbalanceamento das classes, o que pode levar a um recall um pouco menor para a classe 'Churn' (a classe de interesse para interven√ß√£o). N√£o houve sinais claros de overfitting ou underfitting severos com base na avalia√ß√£o do conjunto de teste; no entanto, para uma valida√ß√£o mais robusta, seria ideal empregar t√©cnicas como valida√ß√£o cruzada.\")\n",
        "\n",
        "print(\"\\n## An√°lise de Import√¢ncia das Vari√°veis Preditivas\")\n",
        "print(\"---\")\n",
        "print(\"A compreens√£o das vari√°veis mais influentes √© crucial para a tomada de decis√µes de neg√≥cio:\")\n",
        "\n",
        "print(\"\\n### Regress√£o Log√≠stica (Coeficientes):\")\n",
        "if 'log_reg_model' in locals() and X_scaled is not None:\n",
        "    log_reg_coefficients = pd.Series(log_reg_model.coef_[0], index=X_scaled.columns).sort_values(key=abs, ascending=False)\n",
        "    print(\"Os coeficientes da Regress√£o Log√≠stica (considerando sua magnitude) indicam que as vari√°veis mais impactantes s√£o:\")\n",
        "    print(log_reg_coefficients.head(5).to_markdown())\n",
        "    print(\"Isso refor√ßa que `customer_tenure`, `internet_InternetService_Fiber optic`, `account_Contract_Month-to-month` e `account_Charges_Monthly` s√£o os mais fortes indicadores de churn.\")\n",
        "else:\n",
        "    print(\"N√£o foi poss√≠vel analisar a import√¢ncia das vari√°veis para a Regress√£o Log√≠stica (modelo n√£o treinado ou dados inconsistentes).\")\n",
        "\n",
        "print(\"\\n### Random Forest (Feature Importance):\")\n",
        "if 'rf_model' in locals() and X_no_scale is not None:\n",
        "    rf_importance = pd.Series(rf_model.feature_importances_, index=X_no_scale.columns).sort_values(ascending=False)\n",
        "    print(\"A import√¢ncia das features calculada pelo Random Forest, que geralmente √© mais precisa para modelos baseados em √°rvore, aponta para:\")\n",
        "    print(rf_importance.head(5).to_markdown())\n",
        "    print(\"Confirmando os achados, `customer_tenure` se destaca como a vari√°vel mais importante, seguida por `account_Charges_Total`, `account_Charges_Monthly`, `internet_InternetService_Fiber optic` e `customer_SeniorCitizen`.\")\n",
        "else:\n",
        "    print(\"N√£o foi poss√≠vel analisar a import√¢ncia das vari√°veis para o Random Forest (modelo n√£o treinado ou dados inconsistentes).\")\n",
        "\n",
        "print(\"\\n## Conclus√µes Finais e Recomenda√ß√µes Estrat√©gicas\")\n",
        "print(\"---\")\n",
        "print(\"A an√°lise completa e a modelagem preditiva da evas√£o de clientes da Telecom X levaram a conclus√µes claras sobre os principais fatores de churn e oferecem bases s√≥lidas para estrat√©gias de reten√ß√£o.\")\n",
        "\n",
        "print(\"\\n### Principais Fatores que Afetam a Evas√£o de Clientes:\")\n",
        "print(\"1. **Tempo de Contrato (`customer_tenure`):** √â, de longe, o preditor mais significativo. Clientes com pouco tempo de empresa s√£o os mais vulner√°veis √† evas√£o.\")\n",
        "print(\"2. **Tipo de Contrato (`account_Contract_Month-to-month`):** A flexibilidade do contrato mensal est√° fortemente associada a uma maior probabilidade de churn. Contratos de longo prazo (um ou dois anos) promovem maior lealdade.\")\n",
        "print(\"3. **Servi√ßo de Internet de Fibra √ìptica (`internet_InternetService_Fiber optic`):** Inesperadamente, este servi√ßo premium est√° ligado a uma maior taxa de churn, indicando que, apesar da tecnologia, pode haver problemas de percep√ß√£o de valor, qualidade ou suporte ao cliente.\")\n",
        "print(\"4. **Faturamento Mensal (`account_Charges_Monthly`) e Total Gasto (`account_Charges_Total`):** Clientes com contas mensais mais altas, especialmente aqueles com menor gasto total (novos clientes com planos caros), s√£o mais propensos a churnar.\")\n",
        "print(\"5. **M√©todo de Pagamento (`account_PaymentMethod_Electronic check`):** A associa√ß√£o com o cheque eletr√¥nico sugere um ponto de atrito na jornada do cliente ou um perfil de cliente menos engajado.\")\n",
        "print(\"6. **SeniorCitizen (`customer_SeniorCitizen`):** Clientes idosos mostram uma leve tend√™ncia a churnar. (Isso √© uma observa√ß√£o adicional, se relevante pelos resultados de import√¢ncia).\")\n",
        "\n",
        "print(\"\\n### Propostas de Estrat√©gias de Reten√ß√£o Baseadas nos Insights:\")\n",
        "print(\"1. **Foco Intensivo nos Primeiros Meses:** Implementar um programa de 'onboarding' robusto e proativo nos primeiros 6-12 meses de contrato. Isso pode incluir check-ins regulares, ofertas de personaliza√ß√£o de servi√ßos, e canais de suporte dedicados para resolver quaisquer problemas iniciais rapidamente e construir lealdade.\")\n",
        "print(\"2. **Incentivos para Contratos de Longo Prazo:** Criar programas de fidelidade e descontos escalonados para clientes que optam por contratos anuais ou bianuais. Comunicar claramente as economias e benef√≠cios desses planos em compara√ß√£o com o mensal.\")\n",
        "print(\"3. **Otimiza√ß√£o da Experi√™ncia com Fibra √ìptica:** Realizar pesquisas de satisfa√ß√£o detalhadas e foco em 'senior citizens' para clientes de fibra √≥ptica. Investigar gargalos de servi√ßo, otimizar o suporte t√©cnico para problemas espec√≠ficos da fibra e garantir que o valor percebido corresponda ao custo do servi√ßo.\")\n",
        "print(\"4. **Gest√£o de Valor e Expectativas:** Para clientes com faturamento mensal alto, considerar um 'gerente de contas' ou ofertas personalizadas que justifiquem o custo elevado, como pacotes de servi√ßos adicionais ou upgrades. Monitorar a satisfa√ß√£o desses clientes de perto.\")\n",
        "print(\"5. **Melhoria da Experi√™ncia de Pagamento:** Avaliar e otimizar o processo de pagamento via 'Cheque Eletr√¥nico'. Oferecer incentivos para a migra√ß√£o para m√©todos de pagamento mais automatizados e convenientes, reduzindo o atrito na jornada do cliente.\")\n",
        "print(\"6. **Implementa√ß√£o de Sistema de Alerta de Churn:** Integrar o modelo preditivo (preferencialmente o Random Forest, dada sua performance) em um sistema que identifique clientes de alto risco de churn. Isso permitir√° que a equipe de reten√ß√£o entre em contato com esses clientes proativamente, oferecendo solu√ß√µes personalizadas antes que o cancelamento se concretize.\")\n",
        "print(\"A combina√ß√£o de insights baseados em dados com a√ß√µes estrat√©gicas direcionadas √© a chave para transformar a previs√£o de churn em sucesso de reten√ß√£o.\")\n",
        "\n",
        "print(\"\\n--- FIM DO RELAT√ìRIO ANAL√çTICO ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvAwrj_AbbmD"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
